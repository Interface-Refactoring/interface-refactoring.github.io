var store = [{
        "title": "Hello World!",
        "excerpt":"The first three refactorings have been published:      Inline Information Holder   Introduce Pagination   Extract Operation  ","categories": ["news"],
        "tags": [],
        "url": "/news/hello-world/",
        "teaser": null
      },{
        "title": "Second Bundle of Refactorings Released",
        "excerpt":"Today we are releasing the next bundle of refactorings:      Bundle Requests   Extract Information Holder   Move Operation   Bundle Requests guides you through the steps to introduce the Request Bundle pattern into your API. With Extract Information Holder, we now have the inverse refactoring of the previously published Inline Information Holder. Finally, Move Operation helps you focus the responsibilities of an endpoint on a single role to observe the single responsibility principle.  ","categories": ["news"],
        "tags": [],
        "url": "/news/second-slice-released/",
        "teaser": null
      },{
        "title": "Third Set of API Refactorings Released",
        "excerpt":"Today the next bundle of refactorings went live/public:      Rename Operation   Merge Endpoints   Introduce Data Transfer Object   Rename Operation is the classical rename refactoring applied to API operations. Merge Endpoints helps to bring operations from multiple endpoints together to increase cohesion. Finally, Introduce Data Transfer Object (DTO) is often useful in preparation of other refactorings. For example, when introducing pagination, a DTO provides the structure to transmit the additionally required metadata.   We also updated the following ones substantially:      Move Operation   Introduce Pagination   And provide the authors copy of “From Code Refactoring to API Refactoring: Agile Service Design and Evolution Mirko Stocker and Olaf Zimmermann”, published in the proceedings of SummerSoC 2021 (Springer CCIS).  ","categories": ["news"],
        "tags": [],
        "url": "/news/third-slice-released/",
        "teaser": null
      },{
        "title": "Fourth Set of API Refactorings Released",
        "excerpt":"Today’s release features refactorings with high architectural significance. These are:      Add Wish List   Encapsulate Context Representation   Segregate Commands from Queries   Add Wish List guides you through the steps to introduce the Wish List pattern into your API. Encapsulate Context Representation also applied a pattern to the API design, Context Representation. Finally, Segregate Commands from Queries (a.k.a. Introduce Command Query Responsibility Segregation) explains how to split a single endpoint into two to separate write and read operations so that these can be evolved and improved independently.  ","categories": ["news"],
        "tags": [],
        "url": "/news/fourth-slice-released/",
        "teaser": null
      },{
        "title": "Fifth Slice of API Refactorings Released",
        "excerpt":"In today’s release, we are happy to present you three new API refactorings:      Make Request Conditional   Rename Endpoint   Split Operation   Make Request Conditional is concerned with speeding up the communication between API client and provider by not re-sending data the client already has. This can be done by refactoring an API operation to the Conditional Request pattern. The other two refactorings in this release, Rename Endpoint and Split Operation, aim at improving the maintainability of an API, specifically learnability, understandability, and evolvability.   We also updated several of the previously published refactorings substantially to incorporate feedback and to consolidate the stakeholder concerns and smell sections across refactorings. For example, Introduce Pagination was significantly expanded with more “Hints and Pitfalls to Avoid”, contributed by Andrei Furda.  ","categories": ["news"],
        "tags": [],
        "url": "/news/fifth-slice-released/",
        "teaser": null
      },{
        "title": "Slice 6 available online",
        "excerpt":"Three new API refactorings are now published online:      Add Wish Template   Rename Representation Element   Merge Operations   With today’s release, the previously published Add Wish List gets an alternative refactoring: Add Wish Template. If a plain list that enumerates the desired response data structure elements is not flexible enough, the Wish Template pattern offers a richer alternative where clients can express their wishes in a complex data structure. The new Rename Representation Element refactoring completes the Rename series of Rename Endpoint and Rename Operation. Finally, Merge Operations reverts Split Operation and is eligible when two (or more) operations should be collapsed into a single one.  ","categories": ["news"],
        "tags": [],
        "url": "/news/sixth-slice-released/",
        "teaser": null
      },{
        "title": "Done for 2021",
        "excerpt":"The final four API “refactorings” (for this year) are now published online. Unlike the previously provided ones, they affect service level agreements and desired evolution qualities rather than the endpoint design and the message structures (for the most part):      Introduce Version Identifier   Tighten Evolution Strategy   Relax Evolution Strategy   Introduce Version Mediator   API providers and customers are often in opposing positions when making decisions about API longevity and development. For example, providers might want to increase their customer base while at the same time minimizing the number of different versions they have to run and maintain in production. An individual customer whose use case is covered by the API may not want to be forced to upgrade to new API versions. In such cases, it helps if the provider clearly states their API guarantees. If they have to be adjusted, advice can be found in our new Tighten Evolution Strategy and Relax Evolution Strategy refactorings.   One possible outcome could be that the API will be offered in several versions, with the provider introducing an explicit Version Identifier so that clients can choose which version to use. Depending on API changes, introducing a Version Mediator can (temporarily) hide API changes from clients by introducing a mediator that maps the old API calls and their request- and response messages to new ones.  ","categories": ["news"],
        "tags": [],
        "url": "/news/seventh-slice-released/",
        "teaser": null
      },{
        "title": "Refactorings implemented in MDSL Tools",
        "excerpt":"The latest MDSL Tools implement most refactorings described in this catalog. They are also available through the MDSL Web Tools, a Web application to transform API specifications written in MDSL and generate Open API and other interface descriptions from them. The following screenshot shows the user interface to apply the Introduce Pagination refactoring:      The source code is freely available at GitHub. See the MDSL documentation on Transformations Related to Patterns and Refactorings for details.  ","categories": ["news"],
        "tags": [],
        "url": "/news/refactorings-implemented-in-mdsl/",
        "teaser": null
      },{
        "title": "API Design Patterns Book Published",
        "excerpt":"The API design patterns featured in many of the refactorings in this catalog form the core of the book called “Patterns for API Design: Simplifying Integration with Loosely Coupled Message Exchanges”, published by the Addison-Wesley Signature series editor Vaughn Vernon on Twitter and LinkedIn.   The book features 44 patterns as well as an introduction to API fundamentals and a domain model for APIs. Six decision narratives guide through the conceptual level of API design, identifying 29 recurring decisions with options and criteria. It applies the patterns to three cases, the fictitious Lakeside Mutual microservices scenario as well as two real-world projects that have been running in productions for a long time. The book also contains an introduction to the Microservice Domain Specific Language (MDSL) that we used to illustrate many of our refactorings. A pattern eligibility cheat sheet is available as well.      “Patterns for API Design” is available at Amazon.com, InformIT and other bookstores.  ","categories": ["news"],
        "tags": [],
        "url": "/news/patterns-book-published/",
        "teaser": null
      },{
        "title": "API Design Pattern of the Week",
        "excerpt":"The API Design Pattern of the Week series is a collection of articles introducing the API design patterns featured in the Patterns for API Design book. The series is published on LinkedIn and Medium. A Medium article provides an overview of the series.  ","categories": ["news"],
        "tags": [],
        "url": "/news/api-design-pattern-of-the-week/",
        "teaser": null
      },{
        "title": "Refactorings Workshopped at EuroPLoP23",
        "excerpt":"We submitted eight interface refactorings to this year’s EuroPLoP writer’s workshop. The paper was well received, and we received a lot of helpful and constructive feedback on how to improve it. The overall impression was that it was well written, the figures helped with understanding, and the examples were well chosen and explained. See Mirko’s blog post for more details.   The open-access paper is available for download here.  ","categories": ["news"],
        "tags": [],
        "url": "/news/refactorings-workshopped/",
        "teaser": null
      },{
        "title": "API Design Patterns Adoption and Refactoring Catalog Update",
        "excerpt":"We have been busy with our Interface Refactoring Catalog.  We present the next seven refactorings in Workshop B of EuroPLoP 2024 taking place this week. The first eight interface refactorings were workshopped at EuroPLoP 2023; see our trip report.   The patterns that serve as targets of many of our refactorings are being used in industry and academia. A news item on the API Design Patterns website provides more information.   Our API design patterns book won the “Best Publication Award” 2023/2024 at their university OST – Eastern Switzerland University of Applied Sciences. The jury praised the book for its practical relevance and international impact. We are very proud of this peer recognition!       ","categories": ["news"],
        "tags": [],
        "url": "/news/patterns-adoption-refactorings-update/",
        "teaser": null
      },{
        "title": "More Refactorings Workshopped and Published (EuroPLoP24)",
        "excerpt":"We submitted another seven interface refactorings to this year’s EuroPLoP writer’s workshop. Starting from stakeholder concerns and API design smells was seen to be useful, and the detailed guidance by example appreciated. The carefully chosen names for the refactorings were particularly appreciated for their readability, and the extensive references and footnotes will provide a rich source for their own work. One participant even suggested that it should be made into a book, with more introduction and context for newcomers. Again, we received a lot of helpful and constructive feedback on how to improve the refactorings and their presentation.     The open-access paper “Pattern-oriented API Refactoring: Addressing Design Smells and Stakeholder Concerns”, to appear in the ACM Digital Library, is available for download via the publications page of this website. We hope that we considered all the feedback we received and that the refactorings are now even more useful for practitioners and researchers alike.  ","categories": ["news"],
        "tags": [],
        "url": "/news/more-refactorings-workshopped/",
        "teaser": null
      },{
        "title": "Add Wish List",
        "excerpt":"also known as: Introduce Payload Feature Toggle, Support Response Shaping and Expansion   Context and Motivation   An API offers one or more endpoints exposing operations to retrieve structured data. Not all API clients are interested in the same Data Elements that can be retrieved, and both clients and providers want to reduce unnecessarily transmitted and processed data.   As an API client, I want exact control over response message content so that I receive just the data I require to realize an application feature.   Ideally, a single call should be enough to retrieve all required data. So one solution would be to offer custom operations for different clients, but this requires much knowledge about the clients on the provider side, which might not be easy to obtain and change dynamically. It would also increase maintenance effort and coordination needs.   Stakeholder Concerns      #client-information-needs   APIs are often used by multiple clients with different information needs, which makes it difficult for their providers to offer one-size-fits-all solutions.   #evolvability, #flexibility   The information needs of clients and users vary over time, and an API and its provider-side implementation should not have to be adjusted every time something changes in its clients. Maintaining several variations of the same operation for different clients is possible but increases the maintenance effort and coordination needs, which in turn might constrain the flexibility of the API provider.   #green-software, #performance, #data-parsimony   Overall response time, throughput, client-side and server-side processing time are qualities that concern API clients and their providers. Unused data that is prepared, transported, and processed wastes resources and ought to be avoided.   Initial Position Sketch   The initial response shown in Figure 1 comprises several, possibly nested, Data Elements, also known as attributes.      Figure 1: Add Wish List: Initial Position Sketch. The API provider responds to a request from a client (1) with a message (2) that contains Data Elements. The client does not require all the received data.   The targets of this refactoring are API operations with their request and response messages. The refactoring applies in different situations: the provider might return data the client is not interested in, as shown in Figure 1, where the client wants the ability to filter elements to avoid overfetching. The refactoring is also applicable when the provider omits data the client must retrieve through a second request from a different API operation. Clients can use the Wish List to expand the message content to avoid underfetching in that case.   Design Smells      Underfetching   Clients have to call multiple API operations to get all data they require because these operations do not offer any way to define the targeted Data Elements.   Overfetching   Clients throw away large parts of the received data because the API design follows a one-size-fits-all approach, and the provider includes all data in responses that any present or future client might be interested in. For example, in an e-commerce API, product procurement information might only interest a few clients, while most want to learn about current prices and items in stock. Another phenomenon is “sell what is on the truck”: implementation data is exposed just because it is there, without any client-side use case.   API does not get to the POINT   According to the POINT principles for API design, any operation should have a purpose. It should also be T-shaped (both broad and deep, that is). Underfetching and overfetching indicate that these two principles are violated or only partially met.   Instructions   Adding a Wish List to request messages lets clients select what Data Elements they want the response to contain. Applying this pattern reduces the mismatch between client expectations and provider capabilities:      Analyze the current response message structure with respect to nesting, optionality, and data usage profiles. All its elements to become selectable must be optional; otherwise, not wishing for them cannot have any effect.   Apply Introduce Data Transfer Object (DTO) or adjust existing DTOs as needed.   Add a set-valued request parameter that allows clients to enumerate the desired Data Elements. The name of the parameter could, for example, be called wishList, select, or expand. Decide on a list separator that is easy to transport over the given networking protocol (for instance, slashes “/” might not be a good choice in HTTP APIs, but commas “,” work well).   Populate this parameter when preparing requests on the client side.   Process this parameter when responding to requests on the provider side. Evaluate which Data Elements to return and prepare the response message accordingly. Avoid retrieving data from the data store only to discard it afterward. For example, customize SQL queries to only fetch the required attributes and avoid unnecessary joins.   Complete the refactoring with the following steps that apply to most/all IRC entries:      Test the changes to ensure that the new design works and that the end-to-end capabilities of the API remain unchanged. Use this opportunity to learn about its effectiveness; establish success criteria derived from the stakeholder concerns. For example, performance might be an important metric that can be tracked by a benchmark.   Enhance the external API Description and document the rationale for the new design.   Upgrade the version number (indicating a backward-compatible feature enhancement in this case) and inform the clients when the new version has been released.   Analyze whether the refactoring application has been successful according to the success criteria.   Several approaches to evolution exist. To preserve the original behavior for clients that omit the Wish List and ensure backward compatibility, the API should make the new parameter optional and keep the current response message structure and content. This approach does not break the clients, but they might miss the opportunity to learn about the Wish List and will continue to request the complete response structure. A different, possibly incompatible approach is only to return a minimal response message by default so that clients are forced to state their wishes.   Target Solution Sketch (Evolution Outline)   The Wish List in the operation signature lets clients specify certain Data Elements they are interested in. The provider then tailors the response to the client’s wishes. This solution is sketched in Figure 2. Chapter 7 of [Zimmermann et al. 2022] shows a solution sketch with a dedicated List Evaluator component to handle the client’s wish.      Figure 2: Add Wish List: Target Solution Sketch. In its request (1), the client also sends a Wish List. The provider uses this wish to decide whether some Data Element should be included in the response message (2). The provider implementation might also delegate this filtering to the repository component to avoid retrieving unneeded data from the data store.   Example(s)   The following example from our Lakeside Mutual sample application shows a response of the Policy Management backend microservice before refactoring. Note that no customer master data is included, only a customerId:   curl http://localhost/policies/fvo5pkqerr {   \"policyId\" : \"fvo5pkqerr\",   \"customerId\" : \"rgpp0wkpec\",   \"creationDate\" : \"2021-07-07T13:40:52.201+00:00\",   \"policyPeriod\" : {     \"startDate\" : \"2018-02-04T23:00:00.000+00:00\",     \"endDate\" : \"2018-02-09T23:00:00.000+00:00\"   },   ... }   A second request to the respective endpoint could then be performed to retrieve the customer data. However, the API description specifies that the GET operation may take an optional expand parameter:   '/policies/{customerIdDto}':   get:     tags:       - customer-information-holder     summary: Get a customer's policies.     operationId: getPoliciesUsingGET     produces:       - '*/*'     parameters:       - name: customerIdDto         in: path         description: the customer's unique id         required: true         type: string       - name: expand         in: query         description: a comma-separated list of the fields             that should be expanded in the response         required: false         type: string     ...   Adding expand=customer to the query string results in the following response, which now includes customer master data:   curl http://localhost/policies/fvo5pkqerr?expand=customer {   \"policyId\" : \"fvo5pkqerr\",   \"customer\" : {     \"customerId\" : \"rgpp0wkpec\",     \"firstname\" : \"Max\",     \"lastname\" : \"Mustermann\",     \"birthday\" : \"1989-12-31T23:00:00.000+00:00\",     \"streetAddress\" : \"Oberseestrasse 10\",     \"postalCode\" : \"8640\",     \"city\" : \"Rapperswil\",     \"email\" : \"admin@example.com\",     \"phoneNumber\" : \"055 222 4111\",     \"moveHistory\" : [ ],     ...   },   \"creationDate\" : \"2021-07-07T13:40:52.201+00:00\",   \"policyPeriod\" : {     \"startDate\" : \"2018-02-04T23:00:00.000+00:00\",     \"endDate\" : \"2018-02-09T23:00:00.000+00:00\"   },   ... }   The client does not have to issue a second request for the data when using the Wish List.   Hints and Pitfalls to Avoid   If some of the data that can be referred to in the wishes resides in another API endpoint, the API implementation is now depending on that other endpoint, which could not be permitted or desired according to the overall architecture that has been decided for.   Remember that security measures might have to be adjusted because customers might retrieve more data in a single call than before the refactoring. Measures could include checking for authorization and other quality management concerns, like accounting for Rate Limits.   If providing a single, general-purpose operation for different clients does not feel right, there is nothing wrong with having specialized operations, endpoints, or even entire APIs for clients. See the Backends for Frontend pattern for a deeper discussion of the pros and cons of this approach.   Too many optional parameters can lead to difficult-to-use APIs, impeding learnability and resulting in a complex implementation logic.   Related Content   Step 2 of this refactoring applies Introduce Data Transfer Object. The MDSL Tools contain an implementation of this refactoring.   Split Operation could be used to undo this refactoring: if the Wish List is small (for example, there might only be a single data element that can be selected), it might be better to offer two separate operations instead of one with an optional Wish List parameter.   In the example above, we saw that the entire customer entity was included in the response. If only parts of that data are used, the Wish Template pattern provides a mock object-based approach to further tailor the response to the client’s wishes. A Wish List can be evolved and graduated into such a Wish Template with the Add Wish Template refactoring.   The Known Uses1 section of the Wish List pattern explains variants and implementation options. For example, Atlassian JIRA has a concept of resource expansion.   A Wish List that is introduced with this refactoring can also provide flexibility regarding the content of request messages. For example, when partially updating server-side data, instead of offering many distinct operations that each update a specific field, a single operation can be provided. The request parameters of a Wish List are not used to tailor the response but instruct the endpoint on what data to update. “Practical API Design at Netflix, Part 2: Protobuf FieldMask for Mutation Operations” in the Netflix technology blog shows this in the context of gRPC using Field Masks, the gRPC and Protocol Buffers pendant to our Wish List pattern.   References             Zimmermann, Olaf, Mirko Stocker, Daniel Lübke, Uwe Zdun, and Cesare Pautasso. 2022. Patterns for API Design: Simplifying Integration with Loosely Coupled Message Exchanges. Addison-Wesley Signature Series (Vernon). Addison-Wesley Professional.                        https://www.api-patterns.org/patterns/quality/dataTransferParsimony/WishList#sec:WishList:KnownUses &#8617;           ","categories": [],
        "tags": [],
        "url": "/refactorings/addwishlist",
        "teaser": null
      },{
        "title": "Add Wish Template",
        "excerpt":"also known as: Add Expansion Mock Object, Reify Wish List, Turn Wish List into Wish Template   Context and Motivation   A Wish List might have been introduced to let clients explicitly mark the response data they are interested in (to reduce response message size); it is considered insufficient and/or cumbersome to use.   As an API client developer, I want to express the desired response structure as a mock object so that I can be precise and selective when expressing my wishes about structured response data that I am interested in.   This refactoring can also be applied to improve performance if a Wish List has not been used yet.   Stakeholder Concerns      #client-information-needs   As motivated in Add Wish List, APIs are often used by multiple clients with differing information needs, which makes it difficult for providers to offer one-size-fits-all solutions. Some clients may require overviews of information elements, for instance list of identifiers, others may require a few or all of the data fields/attributes of these elements that the API is able to provide.   #performance   Response time and throughput are qualities that concern both API clients serving end users and API providers (also explained in Add Wish List). Preparing, transporting, and processing data but not using it on the client side wastes resources and ought to be avoided.   #maintainability, #flexibility   Client information needs change over time, but an API and its implementation should not have to be updated every time a client does. Maintaining multiple variants of the same operation for different clients is possible, but increases maintenance cost and coordination effort, which in turn can limit the flexibility of the API provider.   Initial Position Sketch   A client might request and receive nested and or flat response data via one or more operation calls:      Add Wish Template: Initial Position Sketch   Any operation with response data structures that clients want to customize is a potential target.   Design Smells      Underfetching   Clients may have to call multiple API operations to obtain all required data because the request messages of these operations do not offer any way to define the targeted representation elements (thus publishing parts or all of the entities of a domain model and their attributes).   Overfetching   Clients may throw away large parts of the received data because the API design follows a one-size-fits-all approach and includes all data that any present or future client might be interested in (according to the smell description in Add Wish List). Another phenomenon is “sell what is on the truck”: implementation data is exposed just because it is there, without a client-side use case.   Structured artifact serialized and therefore strangled   Some data representation that is nested is serialized into a custom string format that is hard to parse and keeps on causing surprises in testing and production. In object-oriented programming, for instance, objects that contain other objects often have to be mapped to and from text notations such as JSON. The reserved characters of the notations (for instance, curly braces {}and double quotes \" in JSON) have to be escaped during serialization, which can be tedious.   Instructions   Introducing a Wish Template requires careful consideration and enforcement:      Review the structure of the response message of the operation to define the scope of the wishes to be expressed; for instance, all elements must be optional. If necessary, wrap the structure in a Data Transfer Object (DTO) by applying the Introduce Data Transfer Object refactoring. If the request message already contains a Wish List that is supposed to be upgraded and reified, review its syntax and semantics.   Copy the schema definition of the eligible part of the response message to the response message schema to create the mock DTO template. Define which values mark inclusion and exclusion, respectively (for example, \"in\" and \"out\" for strings and 1 and 0 for integers and lists with one or zero entries).   Optionally, change the data type of all atomic elements in the structure (such as strings and integers) to boolean. In this case, true marks inclusion and false marks exclusion. Apply these optional changes to the entire nested/hierarchical data structure recursively: in lists, turn all list elements into booleans; in Parameter Trees, process all child elements recursively.   Replace an existing flat, list-oriented Wish List (if any) with the mock DTO serving as Wish Template. If no such list already exists, add/append the template to the request message.   As for most, if not all, refactorings, apply the TELL steps – test, extend API Description, let clients know, leverage logging – that are generally explained at the bottom of this page). Here, testing edge cases and combinations of input data is particularly important. For instance, test empty wishes and a representative set of tree populations such as entire nested structure requested, only top-level information requested, and only leaves requested.   The revised API design is not necessarily backward-compatible. Making the template parameter optional helps preserve backward compatibility.   Target Solution Sketch (Evolution Outline)   The hierarchical structure of the wish matches the structure of the response data. It is shown in the following figure:      Add Wish Template: Target Solution Sketch   In the online description of the Wish Template pattern, the variant that changes the response data types to boolean is sketched in a UML class diagram:      The template mirrors the output DTO on two nesting levels.   Example(s)   A Wish List for a customer response payload might look as the \"desiredElements\":MD&lt;string&gt;* in the request messages of getCustomerMasterData in the following example. Note that all response elements are optional, which is indicated by the question marks ? (notation: MDSL):   operation getCustomerMasterData with responsibility RETRIEVAL_OPERATION    expecting payload {\"queryParameters\", &lt;&lt;Wish_List&gt;&gt; \"desiredElements\":MD&lt;string&gt;*}     delivering payload {\"name\":D&lt;string&gt;?,        \"address\": {\"street\":D&lt;string&gt;?, \"zip\":D&lt;int&gt;?, \"city\":D&lt;string&gt;}?}   Listing fields or attributes of provider-side resources (in the broadest sense of the word) works fine (and is used in many APIs, both public and community- or solution-internal ones) — as long as their names are unique, and the resource structure does not cause complex navigation in nested, hierarchical name spaces. A downside of this approach is that it does not catch typos and other mistakes in field/attribute names (at least not on the client side).   Type safety and fine-tuning of the wishes can be achieved by upgrading the flat string list to a \"mockCustomer\" Wish Template that mirrors the customer structure appearing in the response:   operation getCustomerMasterData with responsibility RETRIEVAL_OPERATION    expecting payload {     \"queryParameters\",         &lt;&lt;Wish_Template&gt;&gt; \"mockCustomer\": {         \"mockName\":D&lt;string&gt;?,          \"mockAddress\":{\"mockStreet\":D&lt;string&gt;}?,          \"mockZip\":D&lt;int&gt;?,          \"mockCity\":D&lt;string&gt;}?}     delivering payload {\"name\":D&lt;string&gt;?,      \"address\": {\"street\":D&lt;string&gt;?, \"zip\":D&lt;int&gt;?, \"city\":D&lt;string&gt;?}?}   If such copy of the response structure is used, marker values for the basic types have to be defined in the API contract (for instance, -1 for int values and \"\" empty strings and lists). Alternatively, all basic types can be made optional or changed to boolean type:         &lt;&lt;Wish_Template&gt;&gt; \"inclusionMarkers\": {         \"includeName\":D&lt;bool&gt;?,          \"includeAddress\":{\"includeStreet\":D&lt;bool&gt;?}, \"includeCity\":D&lt;bool&gt;}?}    See MAP website for another example of a Wish Template.   Hints and Pitfalls to Avoid   The design of a Wish Template requires more effort than that of a plain list:      Have the template design be reviewed by different client developers before implementing it; apply the 80-20 rule, also known as the Pareto Principle, along the way.   Focus on cardinalities, optionality, and n:m relations in nested/linked data when designing, implementing, and testing the template. For instance, is it possible to include entire subtrees when specifying inclusion of nodes that are neither roots nor leaves?   Resist the temptation to focus on the wish declaration syntax more than on the business and domain logic using it. In other words, do not become a middleware provider/vendor but use existing languages and engines such as GraphQL for advanced scenarios.   Wish Lists and Wish Templates ay be applied to several operations called in sequence. When improving API performance and rightsizing one or more messages, also consider to apply Microservice API Patterns such as Embedded Entity and Linked Information Holder.   Also remember to update security policies so that clients cannot suddenly access more data than they should.   Related Content   An application of Add Wish List may provide the starting point for this refactoring. A Wish Template can also be introduced if no Wish List existed before; the steps dealing with the existing list are simply skipped in this case. A large integration interface to core banking systems that applied the pattern is described in Zimmermann et al. [2004].   GraphQL engines can be seen as a tool- and middleware-supported application of this refactoring, prpviding a single endpoint for flexible queries and mutations. The MDSL Tools contain an implementation of this refactoring.   References             Zimmermann, Olaf, Sven Milinski, Michael Craes, and Frank Oellermann. 2004. “Second Generation Web Services-Oriented Architecture in Production in the Finance Industry.” In Companion to the 19th Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages, and Applications, 283–89. OOPSLA ’04. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1028664.1028772.        ","categories": [],
        "tags": [],
        "url": "/refactorings/addwishtemplate",
        "teaser": null
      },{
        "title": "Bundle Requests",
        "excerpt":"also known as: Introduce Request Bundle   Context and Motivation   The description of Request Bundle sets the pattern context as follows: API “clients make many small requests (in terms of request message size), and individual responses are returned for one or more of these requests. These sequences have a negative impact on scalability and throughput.” (source: Microservice API Patterns (MAP) website)   As an API provider, I want to allow my clients to reduce the number of requests and responses to save bandwidth and network capacity and to avoid unnecessary message processing.   Stakeholder Concerns      #performance   Transferring many small requests may stress network and communication endpoints preparing requests and consuming them (for instance, on HTTP or TCP/IP level).   #developer-experience   Bulk/batch uploads might be prepared by iterating through a data set and then sending API operation calls for each entry in the set. It might be easier to prepare a single technical request message (for instance, a JSON array) that contains several business-, domain-, or application-level requests instead of having to manage the state (running, succeeded, failed) of all individual requests on the client side.   Initial Position Sketch   The API provider currently sends multiple requests (responses not shown in figure):      Bundle Requests: Initial Position Sketch   Targets:      An API operation and its request and response messages.   Design Smells      High latency/poor response time   Responses take a long time to be returned to clients because many small requests cause high workload for the communications infrastructure and protocol endpoint(s) on the provider side.   Atomicity and consistency management issues   Some clients might want to make sure that either all or none of the requests and corresponding responses complete successfully. This can be much harder if they arrive one by one; most remote APIs do not offer global transactions (for instance, distributed two-phase commits) nowadays — for good reasons such as operation overhead, testing effort, and technical risk over time.   Instructions   The refactoring can be applied to an existing operation (as a breaking change):      Apply the Request Bundle Pattern to an existing operation: “Define a Request Bundle as a data container that assembles multiple individual requests in a single request message. Add metadata such as number and identifiers of individual requests.” (source: Microservice API Patterns (MAP))   Adjust the API Description, tests, and all other supporting/supplemental artifacts accordingly.   Alternatively, it can be offered as/in a new operation in the same endpoint:      Copy an existing operation.   Apply the Request Bundle Pattern to the new operation, as already described above.   Document and test the new operation.   Special emphasis should be put on the monitoring to assess the positive or negative effect of the refactoring. Average response times and message sizes should logged, and the performance of the initial API design be compared with that of the refactored one.   Target Solution Sketch (Evolution Outline)   After the refactoring, both the request message and, optionally, the response message of the refactored operation contain collections of individual requests (note that only the request bundling is shown in the figure; the corresponding responses are left out):      Bundle Requests: Target Solution Sketch   Optionally, the bundles can be annotated with additional Metadata Elements. A bundle-level Error Report may be included in the response.   A variant of the Request Bundle pattern is shown in the HTTP resource Customer Information Holder in Lakeside Mutual.   Example(s)   In this example, we will add a Request Bundle to a Customer Relationship Management endpoint:   API description RequestBundleExample  data type YesOrNo D&lt;bool&gt; data type QueryExpression D&lt;string&gt; // detailed query syntax not shown for brevity data type CustomerDTO {\"name\":D&lt;string&gt;, \"contacts\":D&lt;string&gt;*}  endpoint type CustomerRelationshipManagement serves as MASTER_DATA_HOLDER exposes    operation createCustomer with responsibility STATE_CREATION_OPERATION     expecting payload CustomerDTO      delivering payload \"id\":ID&lt;int&gt;        operation updateCustomer     expecting payload CustomerDTO      delivering payload \"ok\":YesOrNo    operation lookupCustomers with responsibility RETRIEVAL_OPERATION     expecting payload \"criteria\":QueryExpression      delivering payload \"results\":CustomerDTO*   Note that the response of the new operation sendBulkRequest contains a tree of objects (just like the request):   API description RequestBundleExample  data type YesOrNo D&lt;bool&gt; data type QueryExpression D&lt;string&gt; // detailed query syntax not shown for brevity  data type CustomerDTO {\"name\":D&lt;string&gt;, \"contacts\":D&lt;string&gt;*}  data type CustomerOperation {      \"createCustomer\":CustomerDTO     |\"updateCustomer\":CustomerDTO     |\"lookupCustomers\":QueryExpression  }  data type CustomerOperationResponse {      \"id\":ID&lt;int&gt;     |\"ok\":YesOrNo     |\"results\":CustomerDTO* }  endpoint type CustomerRelationshipManagement serves as MASTER_DATA_HOLDER exposes    operation sendBulkRequest      expecting payload &lt;&lt;Request_Bundle&gt;&gt; \"operations\":CustomerOperation+      delivering payload \"results\":CustomerOperationResponse+   This MDSL contract is also available here and its OpenAPI Specification pendant here.   Hints and Pitfalls to Avoid   Consider the following:      Apply the Splitter pattern to unbundle the incoming request message.   Leverage parallel programming concepts in the provider implementation. An advanced option is the Map-Reduce pattern.   Decide whether the responses should also be bundled (see solution and discussion of the consequences of the Request Bundle pattern).   Measure and compare: Do the performance gains justify the extra effort and complexity?   Be willing to undo if the new design turns out to be difficult to teach and maintain and the performance gains do not outweigh these negative consequences.   When implemented by a new endpoint, make sure that it is covered by the same security measures as the original endpoint.   As an alternative to exposing batch processing in an API, you may want to consider using a job scheduler rather than bundled API operations.   A deeper discussion of the pros and cons of the Request Bundle pattern is available on the MAP website.   Related Content   Merge Operations can be used in preparation of Bundle Requests if the requests that should be bundled currently target different operations. Merging these into a single operation enables the introduction of a Request Bundle. Note that we do not feature an “Unbundle Requests” refactoring in our catalog at present.   Introduce Pagination is an alternative to improve performance, focussing on responses rather than requests.   The blog post on API design by James Higginbotham titled “API Design Guidance: Bulk and Batch Import” clarifies the difference between batch and bulk processing and gives related advice.   The (still emerging) MDSL tool prototypes contain an implementation of this refactoring.  ","categories": [],
        "tags": [],
        "url": "/refactorings/bundlerequests",
        "teaser": null
      },{
        "title": "Encapsulate Context Representation",
        "excerpt":"also known as: Add Context Metadata to Payload, Extract QoS Information from Headers   Context and Motivation   An API endpoint and its operations have been defined. API client and provider must exchange context information about their interaction, such as the client’s location, Quality-of-Service (QoS) control data, or data used to authenticate, authorize, and bill clients. API client-provider interactions might be part of conversations within and across API endpoints, possibly involving external systems as well.   As a conversation participant, I want to consolidate all context information in a single place and keep it close to the domain data so that clients and providers can prepare and process it along with that data. This also allows switching protocols if that is required to satisfy requirements and constraints that change over time.   Stakeholder Concerns      #developer-experience and #learnability   Accessing protocol headers is different from accessing message payload; different local APIs and/or platform-specific libraries have to be used. Consolidating information in the payload reduces the learning and implementation effort.   #interoperability and #modifiability   Less protocol-specific functionality means fewer changes are required when one protocol is replaced by another.   #security and #auditability   In multiprotocol scenarios, end-to-end security guarantees can only be given and enforced when the information in protocol headers is aggregated and correlated somehow. System and process assurance auditors appreciate if all relevant compliance information can be found in a single place (that is adequately protected) [Julisch et al. 2011].   Initial Position Sketch   Technical metadata such as API Keys, session IDs, or other QoS properties (for example, correlation identifiers, priority levels, time-to-live information, transactional policies, bandwidth requirements, packet-loss tolerance, or latency constraints) travel exclusively in the form of protocol headers. The initial position sketch in Figure 1 shows a response message with a payload and several protocol headers.      Figure 1: Encapsulate Context Representation: Initial Position Sketch. API client and provider exchange a message that contains context information as Metadata Elements in the protocol header.   The refactoring targets request and/or response messages of one or more operations. Operations that form a conversation may or may not appear in the same API endpoint.   Design Smells      Tight coupling to a communication protocol   Most of the network and communication protocols define their own header formats and fields; HTTP is an example. Some of these protocols support custom headers, and others do not. Using protocol-specific headers locks the communication participant in; this can be positive or negative, depending on context and requirements.   Quality-of-Service (QoS) fragmentation and dispersion   Several protocols might be used in conversations, such as HTTP, gRPC, and asynchronous messaging (AMQP). API clients and providers must go to multiple places to gather or produce all required context information, which can be error-prone, time-consuming, and cause technical debt.   Instructions      Design a data structure, the Context Representation, to represent the context information.   Add this data structure to the request and/or response operations of the targeted operations, add DTOs where necessary.   Implement a client-provider message exchange that produces and consumes instances of the new data structure.   Update the API Description with information about the syntax and semantics of the new message payload part. Provide data usage examples in the documentation, including valid and invalid values (or value ranges).   Inform clients about their new options and/or liabilities to work with the new Context Representation.   Note that it might be required to keep the context information in its current place, e.g., in protocol headers, for backward compatibility reasons. In this case, the refactoring allows clients to choose between the old and new ways of providing context information.   Target Solution Sketch (Evolution Outline)   Applying these steps leads to the solution sketched in Figure 2.      Figure 2: Encapsulate Context Representation: Target Solution Sketch. In addition to protocol-specific metadata transported in protocol headers, application-level Metadata Elements are bundled and included in the payload of the response message.   Example(s)   The following MDSL code snippet shows an API endpoint with an operation that expects context information in the headers:   data type KeyValuePair {   \"name\": ID&lt;string&gt;,   \"property\": D&lt;string&gt; }+  endpoint type SampleService    exposes      operation sampleOperationInitial       expecting          headers {           \"apiKey\":ID&lt;int&gt;,            \"sessionId\":D&lt;int&gt;?,            \"otherQosProperties\":             KeyValuePair*          }         payload            \"regularRequestPayload\":D&lt;string&gt;       delivering          payload \"someUnspecifiedResponseData\"   After the context information has been encapsulated, the apiKey, sessionId, and otherQosProperties from the header have moved. They now appear in a Data Transfer Object called RequestContext that is part of the request payload:   data type RequestContext {   \"apiKey\":ID&lt;int&gt;,    \"sessionId\":D&lt;int&gt;?,    \"otherQosProperties\": KeyValuePair* }  data type KeyValuePair {   \"name\": ID&lt;string&gt;,   \"property\": D&lt;string&gt; }+  endpoint type SampleService    exposes          operation sampleOperationTarget       expecting          payload {           &lt;&lt;Context_Representation&gt;&gt;             \"requestContext\": RequestContext,          &lt;&lt;Data_Element&gt;&gt;             \"regularRequestPayload\":D&lt;string&gt;         }       delivering          payload \"someUnspecifiedResponseData\"   The OpenAPI version of the source and target sketch is available here.   The online example for the Context Representation pattern shows a CustomerInformationHolderService whose getCustomerAttributes operation contains an explicit Context Representation in its request message payload.   Hints and Pitfalls to Avoid   Before and when encapsulating context information, make sure to:      Decide whether the context has a local or global scope with respect to operation invocations in one or more API endpoints. The pattern variants discussed in Zimmermann et al. [2022] provide detailed information about this decision.   Decide whether request or response messages should contain a message payload-level Context Representation (or both). Contextualizing requests is more common; for instance, think about client location, API user data, Wish List items, as well as credentials used to authenticate, authorize, and bill clients. Response contexts can also be observed in practice.   Strive for a reusable data structure design across operations (and endpoints, if possible). Prefer de-jure or de-facto industry standards over own creations to define the inner structure of the QoS information in the Context Representation if possible. For example, RFC 7807 [Nottingham and Wilde 2016] defines a standard way to carry problem details in HTTP response messages.   It might be required but not possible to encrypt the data in protocol headers; this would be a reason why this refactoring is eligible. Suppose the payload is encrypted but contains context information used for message routing (for instance, in an API Gateway [Richardson 2018]). In that case, the refactoring might cause undesired decrypt/encrypt steps in the intermediary.   Related Content   Steps 1 and 2 of this refactoring can be seen as an instance of Introduce Data Transfer Object.   See the API Patterns website for a detailed description of the Context Representation pattern.   Undoing the content encapsulation is possible, but our Interface Refactoring Catalog does not contain an explicit inverse refactoring at present.   References             Julisch, Klaus, Christophe Suter, Thomas Woitalla, and Olaf Zimmermann. 2011. “Compliance by Design - Bridging the Chasm Between Auditors and IT Architects.” Computers and Security 30 (September): 410–26. https://doi.org/10.1016/j.cose.2011.03.005.               Nottingham, Mark, and Erik Wilde. 2016. “[Problem Details for HTTP APIs]{.nocase}.” Request for Comments. RFC 7807; RFC Editor. https://doi.org/10.17487/RFC7807.               Richardson, Chris. 2018. Microservices Patterns. Manning.               Zimmermann, Olaf, Mirko Stocker, Daniel Lübke, Uwe Zdun, and Cesare Pautasso. 2022. Patterns for API Design: Simplifying Integration with Loosely Coupled Message Exchanges. Addison-Wesley Signature Series (Vernon). Addison-Wesley Professional.        ","categories": [],
        "tags": [],
        "url": "/refactorings/encapsulatecontextrepresentation",
        "teaser": null
      },{
        "title": "Extract Information Holder",
        "excerpt":"also known as: Extract Resource Representation, Provide Information Holder Link, Share Load, Split Load   Context and Motivation   An API operation returns multiple related, possibly deeply nested data structures to provide clients with a rich dataset in a single response. We call such data elements Embedded Entities [Zimmermann et al. 2020]. For example, in an e-commerce application, the request for the profile of a customer might also return their complete purchasing history. This API is very convenient for clients requiring all the information simultaneously. However, it might not be appropriate for all use cases; some API clients might want to retrieve selected purchasing data through subsequent individual requests when they need it.   As an API client, I prefer to retrieve related data elements step-by-step over having to process large structured data sets appearing in a single response message so that I can process individual responses and the data in them quickly and on demand.   Stakeholder Concerns      #performance, #green-software   Assembling, transferring, and processing a response utilizes resources both on the provider and client side. These resources should not be wasted but handled with care and respect for the environment and the energy consumed. Bandwidth and computing power are examples of valuable and costly resources.   #evolvability, #coupling   Systems and components evolve at different speeds. Hence, they should not depend on each other unless this is justified in the business requirements. Data dependencies often introduce unwanted coupling that is difficult to detect and resolve.   #data-currentness   Data returned by an API might age at different rates. In the e-commerce shop scenario, for instance, the master data of customers (e.g., names, shipping addresses) will change less frequently than transactional data (such as orders). API clients might want to cache some of the data retrieved, which is harder if faster-changing data is embedded in slower-changing data.   #security   Not all API clients have the same access privileges. More fine-grained data Retrieval Operations make it easier to enforce related controls and rules, avoiding the risk that restricted data “slips through” accidentally. To revisit the e-commerce scenario, what if the shop software also includes public ratings of products that show the name and picture of the rating customer? Here, only limited and carefully selected information about the customer should be returned.   Initial Position Sketch   The API implementation shown in Figure 1 returns Data Elements that contain further nested data.      Figure 1: Extract Information Holder: Initial Position Sketch: An API provider responds to a request from a client (1) with a message (2) that contains several, possibly nested, Data Elements. The client does not require all the received data.   The refactoring targets response messages in API operations that return rich data representation elements.   Design Smells      God endpoint   The endpoint offering this operation might have to access many data sources or backend systems to assemble the response. Derived from the “God Class” smell in object-oriented design, the term describes a class or an object that controls numerous other system parts [Riel 1996]. Many such dependencies on external systems and data make the API implementation harder to operate and evolve.   Data lifetime mismatches   Conflating Data Elements with different lifetimes makes caching, especially cache invalidation, harder. This may happen when slow-changing master data contains fast-changing transactional data (for example, in an Operational Data Holder), but also if transactional data that is often refreshed by clients contains embedded master data that infrequently changes.   Overfetching   Clients throw away parts of the received data because the API design follows a one-size-fits-all approach, and the provider includes all data in responses that any present or future client might be interested in. For example, in an e-commerce API, product procurement information might only interest a few clients, while most want to learn about current prices and items in stock.   Sell what is on the truck   Implementation data is exposed just because it is there, without any client-side use case.   Instructions   As a preparation for the refactoring, make sure that the following preconditions are met:      Decide on which parts of the message to extract. See the Embedded Entity and Linked Information Holder patterns for advice [Zimmermann et al. 2022].   Ensure the API offers a dedicated Retrieval Operation for the data that is currently embedded and will be extracted. If this is not already the case, apply the Split Operation refactoring first. An Extract Operation or Segregate Commands from Queries refactoring might also be appropriate to avoid the god endpoint smell.   (Optional) If the API operation does not already use a dedicated Data Transfer Object (DTO), apply the Introduce Data Transfer Object refactoring to decouple the API response message from the internal data model. The presence of a DTO allows changing the response message structure without affecting the internal data model. Depending on how deep the Embedded Entity is nested in the response data structure, the Introduce Data Transfer Object refactoring may have to be applied several times. You might be using a programming language or framework where this step is not required. In that case, you can just skip it as long as you have a means to modify the response message structure.   Replace an Embedded Entity with a Linked Information Holder in the following steps:      Add a Link Element to the response message that points clients to a Retrieval Operation in an Information Holder Resource. This link realizes/applies the Linked Information Holder pattern; when a DTO is present, it is placed in it.   Adjust the tests to the new response structure and run them to observe the changed responses.   (Optional) Deprecate or remove the Embedded Entity in the original response message.   Clean up the implementation code. For example, services, utilities, or repositories previously used to retrieve the embedded data might not be required anymore here; hence, they should either be moved or removed.   Check security policies to ensure that clients can access the linked data.   Adjust API clients under your control to issue additional API calls to retrieve the data available at the endpoint referenced in the new Link Element as needed.   Update API Description [Lübke et al. 2019a], version number, sample code, tutorials, etc., as required. API directories and gateways might have to be updated as well.   Target Solution Sketch (Evolution Outline)   The client can use the Link Element returned in response to the initial request to retrieve the related data in a follow-up call, as shown in Step 3 in Figure 2.      Figure 2: Extract Information Holder: Target Solution Sketch: An API client requests (1) a resource from a provider, which responds with a message (2) containing a Linked Information Holder. The client can then request (3) this data when it needs this data. The provider responds (4) with a Data Element that was embedded in the response in the Initial Position Sketch.   To reap the full benefits of this refactoring, backward compatibility has to be given up. In the first step, the Embedded Entity could be marked as deprecated to give the clients time to adjust. At a time defined and announced when applying the refactoring, the Embedded Entity is removed from the message payload. The Limited Lifetime Guarantee pattern in Lübke et al. [2019b] describes this lifecycle management strategy in detail.   Example(s)   The following API Description shows an endpoint to retrieve a CustomerProfileDTO, which includes the Embedded Entity PurchaseOrderDTOs.   API description ECommerceAPI  data type CustomerProfileId {\"id\": ID&lt;string&gt;}  data type CustomerProfileDTO {   \"id\": CustomerProfileId,   \"name\": Data&lt;string&gt;,   &lt;&lt;Embedded_Entity&gt;&gt; \"purchaseHistory\": PurchaseOrderDTO* }   data type PurchaseOrderDTO \"DTODesignToBeContinued\"   endpoint type CustomerProfileEndpoint  serves as INFORMATION_HOLDER_RESOURCE exposes    operation getCustomerProfile      with responsibility RETRIEVAL_OPERATION     expecting payload CustomerProfileId     delivering payload CustomerProfileDTO      API provider ECommerceAPIProvider   offers CustomerProfileEndpoint  API client ECommerceClient   consumes CustomerProfileEndpoint   This example uses the MDSL notation introduced in Zimmermann et al. [2022].   Having applied the refactoring, the client will now receive a link (notice the purchaseHistory link in CustomerProfileDTO):    data type CustomerProfileDTO {   \"id\": CustomerProfileId,   \"name\": Data&lt;string&gt;, ---  &lt;&lt;Embedded_Entity&gt;&gt; \"purchaseHistory\": PurchaseOrderDTO* +++  &lt;&lt;Linked_Information_Holder&gt;&gt;  +++    \"purchaseHistory\": Link&lt;string&gt; }   data type PurchaseOrderDTO \"DTODesignToBeContinued\"  +++ endpoint type PurchaseHistoryEndpoint +++ serves as INFORMATION_HOLDER_RESOURCE +++ exposes +++   operation getPurchaseHistory +++     with responsibility RETRIEVAL_OPERATION +++     expecting payload CustomerProfileId +++     delivering payload PurchaseOrderDTO*      API provider ECommerceAPIProvider   offers CustomerProfileEndpoint  +++   offers PurchaseHistoryEndpoint   Hints and Pitfalls to Avoid   Comparing the Target Solution Sketch from Figure 2 with the Initial Position Sketch shown in Figure 1 shows that the first resource now accesses fewer repositories to assemble the response message. This enables further architectural refactorings such as Split Application Backend.   Monitor the API to maintain and challenge the rationale for pattern usage. If most or all client calls follow the given Linked Information Holder, consider embedding the target element in the original representation again using the Inline Information Holder refactoring. A deeper discussion of the benefits and liabilities of the two patterns involved in this refactoring and its inverse, Embedded Entity and Linked Information Holder, can be found in the pattern texts in Zimmermann et al. [2022].   For the specific question of whether it is preferable to exchange several small messages or a few larger ones, please refer to our article What is the Right Service Granularity in APIs?   Related Content   The inverse API refactoring is Inline Information Holder.   If there is no operation to retrieve the linked data yet, the Split Operation refactoring can be used to create one.   After a Split Operation refactoring, Extract Information Holder can be used to further “split” the response messages of the operations.   The Wish List and Wish Template patterns (and related Add Wish List and Add Wish Template refactorings) offer alternative solutions to the problem of how an API client can inform the API provider at runtime about the data it is interested in.   Context Mapper [Kapferer and Zimmermann 2020], a modeling framework and Domain-specific Language (DSL) for Domain-Driven Design (DDD), implements a refactoring called Split Aggregate by Entity. A DDD Aggregate [Evans 2003] establishes a transactional boundary around a group of Entities that are persisted together; a data-centric Aggregate could be exposed via an Information Holder Resources on the API level. Splitting such an Aggregate therefore can be seen to correspond to splitting or extracting parts from an API-level Information Holder Resource.   As another example not related to APIs but Web application frontend design, consider the difference between single and multi-page websites. All information is available on a single page regardless of whether it is relevant to each reader. In a multi-page design, the home page gives an overview, and additional information is provided via hyperlinks that can be followed on demand.   References             Evans, Eric. 2003. Domain-Driven Design: Tacking Complexity in the Heart of Software. Addison-Wesley.               Kapferer, Stefan, and Olaf Zimmermann. 2020. “Domain-Driven Service Design.” In Service-Oriented Computing, edited by Schahram Dustdar, 189–208. Springer International Publishing. https://doi.org/10.1007/978-3-030-64846-6_11.               Lübke, Daniel, Olaf Zimmermann, Cesare Pautasso, Uwe Zdun, and Mirko Stocker. 2019a. “Interface Evolution Patterns: Balancing Compatibility and Extensibility Across Service Life Cycles.” In Proceedings of the 24th European Conference on Pattern Languages of Programs. EuroPLop ’19. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3361149.3361164.               ———. 2019b. “Interface Evolution Patterns: Balancing Compatibility and Extensibility Across Service Life Cycles.” In Proceedings of the 24th European Conference on Pattern Languages of Programs. EuroPLop ’19. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3361149.3361164.               Riel, Arthur J. 1996. Object-Oriented Design Heuristics. Reading, MA: Addison-Wesley.               Zimmermann, Olaf, Mirko Stocker, Daniel Lübke, Cesare Pautasso, and Uwe Zdun. 2020. “Introduction to Microservice API Patterns (MAP).” In Joint Post-Proceedings of the First and Second International Conference on Microservices (Microservices 2017/2019), edited by Luı́s Cruz-Filipe, Saverio Giallorenzo, Fabrizio Montesi, Marco Peressotti, Florian Rademacher, and Sabine Sachweh, 78:4:1–17. OpenAccess Series in Informatics (OASIcs). Dagstuhl, Germany: Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik. https://doi.org/10.4230/OASIcs.Microservices.2017-2019.4.               Zimmermann, Olaf, Mirko Stocker, Daniel Lübke, Uwe Zdun, and Cesare Pautasso. 2022. Patterns for API Design: Simplifying Integration with Loosely Coupled Message Exchanges. Addison-Wesley Signature Series (Vernon). Addison-Wesley Professional.        ","categories": [],
        "tags": [],
        "url": "/refactorings/extractinformationholder",
        "teaser": null
      },{
        "title": "Extract Operation",
        "excerpt":"also known as: Split Endpoint   Context and Motivation   One or more API endpoints, for instance, HTTP resources, have been developed, tested, and deployed. One of these endpoints offers multiple operations for clients to call. These operations work with multiple domain concepts. Their functional and technical responsibilities differ regarding stakeholder groups (users, developers, etc.) and their addressed quality concerns. Some operations are process- or activity-oriented, while others offer data storage. At least one operation differs concerning read and/or write access characteristics, access control policies, and data protection requirements. As a consequence, the endpoint serves multiple roles in the API architecture. The operations differ regarding their evolution (e.g., the frequency of changes requiring/leading to new releases).   As the API provider, I want to focus the responsibilities of an endpoint on a single role so that API clients serving a particular stakeholder group understand the API design intuitively, and the release roadmap and scaling of the endpoint can be optimized for each group of stakeholders and clients.   Stakeholder Concerns      #reliability and #stability   Independent endpoints that do not share the same execution context and resources can be deployed independently; operations co-located in a single endpoint, however, share their deployment characteristics. For instance, if a long-running operation causes an API provider-internal error, its sibling operations might suffer from quality-of-service degradations as well. Nygard [2018] uses the term stability: “A robust system keeps processing transactions, even when transient impulses, persistent stresses, or component failures disrupt normal processing.”   #single-responsibility-principle   Architectural principles are affected positively or negatively when APIs are refectored. Here, the Purposeful, style-Oriented, Isolated, channel-Neutral, and T-shaped (POINT) principles for API design apply; extracting an endpoint can improve P, O, and I (but might harm T when looking at a single endpoint and not an entire API).   #independent-deployability, #scalability   Endpoints can be deployed and then scaled separately, which is one of the defining tenets of microservices-based systems [Zimmermann 2017]. The fewer operations an endpoint exposes, the easier it is to optimize the scaling for those operations.   #security   With multiple operations co-located within a single endpoint, it can be challenging to enforce fine-grained access control policies. Refactoring this endpoint into multiple specialized ones allows for more granular control over access permissions and authorization rules. The security requirements for the data an API endpoint exposes may also differ; hence, separating operations can make it easier to apply data protection measures that ensure confidentiality.   Initial Position Sketch   The design for this interface refactoring looks as follows (notation: MDSL):   endpoint type SomeEndpoint exposes    operation operation1      expecting payload \"RequestMessage1\"      delivering payload \"ResponseMessage1\"    operation operation2      expecting payload \"RequestMessage2\"      delivering payload \"ResponseMessage2\"   See Figure 1 for a graphical representation of this Initial Position Sketch.      Figure 1: Extract Operation: Initial Position Sketch. A client uses two different operations (message exchanges 1-2 and 3-4) in an API endpoint for its communication with the API.   The refactoring targets are an API endpoint (for instance, an HTTP resource identified with a URI) and one of its operations (for instance, an HTTP verb/method supported by the resource).   Design Smells      Role and/or responsibility diffusion   The endpoint is both an Information Holder Resource and a Processing Resource, or an Information Holder Resource exposes different types of data in different operations (for instance, both master data and operational data). The endpoint operations may have rather diverse functional and technical responsibilities (read vs. write, for instance). As a consequence of one or more of these smells, it is hard to explain the endpoint purpose.   Low cohesion   The operations in the endpoint deal with multiple, not necessarily related domain concepts. Consequently, the endpoint has more than one reason to change during its evolution. It serves multiple stakeholder groups and/or its implementation is developed and maintained by multiple teams.   REST principle(s) constraints   A key design constraint imposed by the REST style used by many HTTP APIs is the “unified interface,” which mandates the use of standard HTTP verbs (POST, GET, PUT, PATCH, DELETE, etc.). These verbs come with certain restrictions; for instance, GET and PUT operations should be idempotent. Sometimes, REST constraints limit extensibility when a resource identified by a single URI runs out of verbs for its operations [Serbout et al. 2022].   God endpoint   The endpoint and its operations implementations might have to access many data sources or backend systems to assemble responses to requests. Many such dependencies on external systems and data make the API implementation more complicated to operate and evolve. In object-oriented design, a class or object that controls many other system parts is called a “God Class” [Riel 1996].   Wrong cuts   The endpoint might have been designed to serve multiple purposes, and the operations might have been chosen to be co-located in the same endpoint. This design decision might have been made based on the wrong assumptions or requirements, leading to a design that is hard to maintain and evolve.   Instructions   Follow these steps to extract an endpoint:      Remove the operation from the API Description of the source endpoint.   Check the general security policies and the client rights management. For example, authorization rules that use endpoint existence and names to determine whether a client application and end-user are permitted to perform an operation might have to be adjusted.   Refactor at the code level. For instance, create an additional REST controller class when working with Java and HTTP in Spring and move the implementation of the chosen operation.   Create an API Description for the new endpoint that only exposes the extracted operation.   Adjust the existing integration tests or add additional ones to verify that the original and new endpoints meet their API Descriptions (both in terms of functional and non-functional characteristics).   Evaluate whether the roles and responsibilities of the two endpoints are well-separated and that the refactoring resulted in endpoints with higher cohesion.   Inform all API clients about the change and the version that will introduce it. Provide migration information (or support the transition on a technical level, for instance, with an HTTP redirect [Fielding and Reschke 2014]).   If necessary, repeat these steps with the remaining operations until the roles and responsibilities of the endpoint have been clarified and the smells resolved.   Target Solution Sketch (Evolution Outline)   The following simple and abstract MDSL sketch specifies the result of the refactoring at an abstract level (see Figure 2 for a graphical representation):   endpoint type SomeEndpoint exposes    operation operation1      expecting payload \"RequestMessage1\"      delivering payload \"ResponseMessage1\"  endpoint type ExtractedNewEndpoint     exposes operation operation2      expecting payload \"RequestMessage2\"      delivering payload \"ResponseMessage2\"      Figure 2: Extract Operation: Target Solution Sketch. The two conversations (message exchanges 1-2 and 3-4) with the API now go to operations residing in two different API endpoints.   Note that this sketch does not show signs of bad smells in terms of semantics or qualities; the following example does.   Example   Sometimes, it makes sense to separate commands from queries (see Segregate Commands from Queries). This refactoring is a particular case of endpoint extraction. Hence, the following example can be seen as an example of both Segregate Commands from Queries and Extract Operation. It starts from a Domain-Driven Design (DDD) featuring a single Aggregate [Evans 2003].   Aggregate PublicationEndpoint {   Service PublicationManagementFacade {     // a state creation/state transition operation:     @PaperId add(@PublicationEntryDTO newEntry);              // retrieval operations:     @PublicationArchive dumpPublicationArchive();     Set&lt;@PublicationEntryDTO&gt;       lookupPublicationsFromAuthor(String author);     String exportAsBibtex(@PaperId paperId);              // computation operations (stateless):     String convertToBibtex(@PublicationEntryDTO entry);    } }   The notation in the above snippet is Context Mapper Domain-Specific Language (CML) [Kapferer and Zimmermann 2020]. Context Mapper is a modeling framework for DDD that provides a domain-specific language. DDD can be seen as a form of pattern-oriented, object-oriented analysis and design; “Design Practice Reference” contains introductions to tactic and strategic DDD [Zimmermann and Stocker 2021].   This single publication management Aggregate (and API endpoint) can be split into two, leading to this design:   Aggregate PublicationCommandsEndpoint {   Service PublicationManagementCommandFacade {     // a state creation/state transition operation:     @PaperId add(@PublicationEntryDTO newEntry);      // computation operations (stateless):     String convertToBibtex(@PublicationEntryDTO entry);    } }  Aggregate PublicationQueriesEndpoint {   Service PublicationManagementQueryFacade {     // retrieval operations:     @PublicationArchive dumpPublicationArchive();     Set&lt;@PublicationEntryDTO&gt;       lookupPublicationsFromAuthor(String author);     String exportAsBibtex(@PaperId paperId);   } }   Note that this design violates principles such as single responsibility, high cohesion, and low coupling because Bibtex-related operations appear in both endpoints. In response, the Move Operation refactoring can be applied on convertToBibtex. A third endpoint that exposes the two BibTeX-related operations can also be introduced.   Hints and Pitfalls to Avoid   When applying this refactoring, API designers have to make sure that:      Concurrent access to business logic and database from two presentation layers, a.k.a. API endpoints, does not cause issues such as lost updates, phantom reads, deadlocks, and so on [Fowler 2002].   Performance and independent deployability improve as desired (loose coupling of the original and new endpoint). Extracting an endpoint to focus on a single role redistributes the existing responsibilities and logic across multiple endpoints. This redistribution could affect the performance of the API, especially if there are increased interdependencies or additional network calls are introduced. Proper load testing and performance analysis should be conducted to ensure that the refactored API can handle the expected workload and achieve satisfactory response times.   Maintainability does not suffer because of design erosion, duplication of Published Language [Evans 2003], and so on. The refactored endpoints may depend on other services or resources within the system. It is essential to carefully manage and coordinate these dependencies to ensure the refactored endpoints can operate independently and reliably.   Related Content   The Extract Information Holder refactoring can be applied in preparation for this refactoring.   When following the Backends For Frontends pattern, it might be helpful to extract an endpoint to serve a particular frontend.   This refactoring is reverted by Merge Endpoints. Segregate Commands from Queries describes endpoint extraction for a particular reason. Move Operation has a similar purpose and nature but does not create a new endpoint.   The Strangler Fig Application pattern describes an approach to migrating a legacy system incrementally by replacing specific functionality with new applications and services instead of replacing it immediately. The Extract Operation refactoring applied to the strangled legacy system can support such an approach. A backend system exposing multiple service endpoints is generally easier to update incrementally (and replace eventually) than a more monolithic one. The blog post “Refactoring Legacy Code with the Strangler Fig Pattern” provides detailed step-by-step explanations.   References             Evans, Eric. 2003. Domain-Driven Design: Tacking Complexity in the Heart of Software. Addison-Wesley.               Fielding, Roy T., and Julian Reschke. 2014. “Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content.” Request for Comments. RFC 7231; RFC Editor. https://doi.org/10.17487/RFC7231.               Fowler, Martin. 2002. Patterns of Enterprise Application Architecture. Boston, MA, USA: Addison-Wesley Longman Publishing Co., Inc.               Kapferer, Stefan, and Olaf Zimmermann. 2020. “Domain-Driven Service Design.” In Service-Oriented Computing, edited by Schahram Dustdar, 189–208. Springer International Publishing. https://doi.org/10.1007/978-3-030-64846-6_11.               Nygard, Michael T. 2018. Release It! Design and Deploy Production-Ready Software. 2nd ed. Raleigh, NC: Pragmatic Bookshelf. https://learning.oreilly.com/library/view/release-it-2nd/9781680504552/.               Riel, Arthur J. 1996. Object-Oriented Design Heuristics. Reading, MA: Addison-Wesley.               Serbout, Souhaila, Cesare Pautasso, Uwe Zdun, and Olaf Zimmermann. 2022. “From OpenAPI Fragments to API Pattern Primitives and Design Smells.” In 26th European Conference on Pattern Languages of Programs. EuroPLoP’21. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3489449.3489998.               Zimmermann, Olaf. 2017. “Microservices Tenets.” Comput. Sci. Res. Dev. 32 (3-4): 301–10. https://doi.org/10.1007/S00450-016-0337-0.               Zimmermann, Olaf, and Mirko Stocker. 2021. Design Practice Reference - Guides and Templates to Craft Quality Software in Style. LeanPub. https://leanpub.com/dpr.        ","categories": [],
        "tags": [],
        "url": "/refactorings/extractoperation",
        "teaser": null
      },{
        "title": "Inline Information Holder",
        "excerpt":"also known as: Inline Resource Representation, Embed Entity   Context and Motivation   An API provides several endpoints that give clients access to data-centric Information Holder Resources. The resources are related and refer to each other, for instance, via hyperlinks. For example, operational data such as an order in an e-commerce shop may reference a Master Data Holder describing the products.   Clients of the API are interested in the data of several of these linked Information Holder Resources. To access this distributed data, the clients have to send multiple requests.   As the API provider, I want to reduce indirection by embedding data that is available from one or more referenced Information Holder Resources so that my clients have to issue fewer requests when working with linked data.   Stakeholder Concerns      #performance, #green-software   Both API clients as well as providers are interested in keeping the latency and bandwidth usage low and using as few resources as possible.   #usability, #developer-experience   An API that provides clients all the required data with as few requests as possible may be easier to use than an API where the client has to issue many requests and requires complex state management on the client side to keep track of multiple API calls. See the blog post API Design Review Checklist: Questions Concerning the Developer Experience (DX) for hints on improving the developer experience.   #offline-support   When the connection is unstable or if one wants to build offline functionality into an app, one large request is often better than many small ones.   Initial Position Sketch   The initial response shown in Figure 1 contains a Link Element that refers to another information holder of the API. The client has to follow the link to retrieve the data of the referenced resource.      Figure 1: Inline Information Holder: Initial Position Sketch: A response message (2) of an API operation that is requested (1) contains a link to a secondary resource that the client has to retrieve using a follow-up request (3, 4).   In terms of the API specification, the response Data Transfer Object (DTO) PolicyDto contains the customerId of the referenced customer and a link to it (notation: OpenAPI Specification, simplified for brevity):   paths:   '/policies/{policyId}':     get:       summary: Get a single policy.       parameters:         - name: policyId           in: path           description: the policy's unique id           required: true           type: string       responses:         '200':           description: A single policy.           schema:             $ref: '#/definitions/PolicyDto' definitions:   PolicyDto:     type: object     properties:       ...       customerId:         type: string       link:         type: string   The response message uses a DTO to transfer the data.   This refactoring targets an operation in an endpoint and its response message.   Design Smells      Underfetching   Clients have to issue many requests to get the required data, harming performance.   Leaky encapsulation   The implementation data model is leaking through the API. For example, a relational database has been exposed via an API with an endpoint for each table in the database, and now clients must resolve the foreign key relationships between tables.   Instructions   Instead of providing clients with a hyperlink to fetch the related data, the response message of the API operation includes the referenced data:      Decide which linked data to inline/embed into the message. For a discussion of the tradeoffs involved, see the Embedded Entity and Linked Information Holder patterns [Zimmermann et al. 2022].   To transfer the data, insert a new attribute to the DTO.   Retrieve the additional entity or value from the repository and add it to the DTO instance.   If present, e.g., when using Hypertext as the Engine of Application State (HATEOAS), remove the superfluous link to the resource whose data is now inlined. Only perform this removal if backward compatibility is not needed.   Adjust the tests to the new response structure.   Clean up the implementation code if necessary (observing the “Rule of Three” of refactoring1), for example, by moving duplicated code to a common location.   Adjust API clients under your control to remove obsolete API calls, but find and use the inlined data instead.   Adjust API Description, version number, sample code, tutorials, etc., as needed.   The link to the referenced resource can be kept in the response message to maintain backward compatibility. In this case, old clients can still follow the link, and updated clients can use the inlined data directly.   Target Solution Sketch (Evolution Outline)   After the refactoring, the linked information is included in the initial response, saving the client an additional request. This solution is sketched in Figure 2.      Figure 2: Inline Information Holder: Target Solution Sketch: The client requests (1) a resource through the API. The API implementation responds with a rich response message (2) that contains all the data.   The implementation effort on the client also decreases: state management is less complex when fewer requests are needed to fetch required data. These benefits are countered by increased message size, leading to longer transfer times and higher processing and database retrieval effort for the endpoint, which might not be needed by clients after all.   Regarding the API specification, the response DTO now contains the additional data (see the lines at the bottom marked with +++). The link to the referenced resource can be removed if backward compatibility is not needed (see the lines marked with ---).   paths:   '/policies/{policyId}':     get:       summary: Get a single policy.       parameters:         - name: policyId           in: path           description: the policy's unique id           required: true           type: string       responses:         '200':           description: A single policy.           schema:             $ref: '#/definitions/PolicyDto' definitions:   PolicyDto:     type: object     properties:       ...       customerId:         type: string  +++  customer:  +++    type: object  +++      properties:  +++        customerId:  +++          type: string  +++        firstname:  +++          type: string  +++        lastname:  +++          type: string  ---  link:  ---    type: string   Example(s)   The Policy Management backend microservice of Lakeside Mutual, a fictitious insurance company, contains an endpoint to retrieve the details of a specific policy, along with a reference to the customer through their customerId. The following listing shows two requests made using the curl command line tool. It sends an HTTP GET request to the specified URL. The response to this request is a JSON object.   curl http://localhost/policies/fvo5pkqerr  {   \"policyId\" : \"fvo5pkqerr\",   \"customerId\" : \"rgpp0wkpec\",   \"creationDate\" : \"2021-07-07T13:40:52.201+00:00\",   \"policyPeriod\" : {     \"startDate\" : \"2018-02-04T23:00:00.000+00:00\",     \"endDate\" : \"2018-02-09T23:00:00.000+00:00\"   },   ... }  curl http://localhost/customers/rgpp0wkpec  {   \"customerId\" : \"rgpp0wkpec\",   \"firstname\" : \"Max\",   \"lastname\" : \"Mustermann\",   ... }   We start the refactoring by adding a new attribute to the DTO:   public class PolicyDto extends RepresentationModel {     private String policyId; --- private String customerId; +++ private CustomerDto customer;     private Date creationDate; ...   Depending on the backward compatibility requirements, the customerId can be kept in the DTO. Otherwise, it can be removed, as shown above. To fetch the data for the customer, the endpoint implementation uses the customerService, a Java class residing in the business logic layer of the sample application, to look up the customer and add it to the response DTO:   @ApiOperation(value = \"Get a single policy.\") @GetMapping(value = \"/{policyId}\") public ResponseEntity&lt;PolicyDto&gt; getPolicy(     @ApiParam(value = \"the policy's unique id\")     @PathVariable PolicyId policyId) {     logger.debug(\"Fetching policy with id '{}'\",        policyId.getId());     Optional&lt;PolicyAggregateRoot&gt; optPolicy =        policyRepository.findById(policyId);     PolicyAggregateRoot policy = optPolicy.get();     PolicyDto response = PolicyDto.fromDomainObject(policy); +++ CustomerDto customer = customerService. +++   getCustomer(policy.getCustomerId()); +++ response.setCustomer(customer);     return ResponseEntity.ok(response); }   Note that we use the Java Web framework Spring Boot in this example. The annotations @GetMapping and @PathVariable are used to instruct Spring that this is an HTTP endpoint that expects a PolicyId in its path. The annotations prefixed with @Api are used to generate an OpenAPI Specification file from the source code and serve as further documentation.   The customer data is now part of the response message. The client can access the data without issuing a second request:   curl http://localhost/policies/fvo5pkqerr  {   \"policyId\" : \"fvo5pkqerr\",   \"customer\" : {     \"customerId\" : \"rgpp0wkpec\",     \"firstname\" : \"Max\",     \"lastname\" : \"Mustermann\",     ...   },   \"creationDate\" : \"2021-07-07T13:40:52.201+00:00\",   \"policyPeriod\" : {     \"startDate\" : \"2018-02-04T23:00:00.000+00:00\",     \"endDate\" : \"2018-02-09T23:00:00.000+00:00\"   },   ... }   Hints and Pitfalls to Avoid   The referenced information holder should be part of the same API endpoint. Otherwise, performing the refactoring might introduce undesired dependencies between backend services.   An API endpoint may now interact with more backends or databases than before. This additional dependency might not be desired from a separation of concerns standpoint, for instance, when considering role- or attribute-based authorization [Kapferer and Jost 2017].   See the Embedded Entity and Linked Information Holder patterns for a deeper discussion of the benefits and liabilities of each pattern.   Related Content   Extract Information Holder inverses this refactoring.   The Wish List and Wish Template patterns both offer alternative solutions to the problem of how an API client can inform the API provider at runtime about the data it is interested in, known as response shaping.   The Backends For Frontends pattern by Sam Newman is another approach to tailoring a backend to the specific needs of a client.   As also mentioned in the inverse refactoring Extract Information Holder, Context Mapper [Kapferer and Zimmermann 2020] implements these refactorings on domain-level (DDD). While Extract Information Holder corresponds to Split Aggregate by Entity, Inline Information Holder would be established with Merge Aggregates in Context Mapper and DDD.   References             Fowler, Martin. 2018. Refactoring: Improving the Design of Existing Code. 2nd ed. Addison-Wesley Signature Series (Fowler). Boston, MA: Addison-Wesley.               Kapferer, Stefan, and Samuel Jost. 2017. “[Attributbasierte Autorisierung in einer Branchenlösung für das Versicherungswesen - Analyse, Konzept und prototypische Umsetzung]{.nocase}.” Bachelor Thesis, https://eprints.ost.ch/id/eprint/602/: University of Applied Sciences of Eastern Switzerland (HSR FHO).               Kapferer, Stefan, and Olaf Zimmermann. 2020. “Domain-Driven Service Design.” In Service-Oriented Computing, edited by Schahram Dustdar, 189–208. Springer International Publishing. https://doi.org/10.1007/978-3-030-64846-6_11.               Zimmermann, Olaf, Mirko Stocker, Daniel Lübke, Uwe Zdun, and Cesare Pautasso. 2022. Patterns for API Design: Simplifying Integration with Loosely Coupled Message Exchanges. Addison-Wesley Signature Series (Vernon). Addison-Wesley Professional.                        The Rule of Three states that when you copy and paste code for the third time, you should extract it into a method [Fowler 2018]. Not to be confused with the Rule of Three of the Patterns community: call it a pattern if there are at least three known uses (https://wiki.c2.com/?RuleOfThree). &#8617;           ","categories": [],
        "tags": [],
        "url": "/refactorings/inlineinformationholder",
        "teaser": null
      },{
        "title": "Introduce Data Transfer Object",
        "excerpt":"also known as: Map and Wrap Representation Structure, Ubiquitous Language Wrapper   Context and Motivation   An API offers one or more operations that return Data Elements [Zimmermann et al. 2022]. For example, the API of a customer relationship management service might contain an endpoint that returns detailed information about customers and the interactions with them. The structure of these responses might have been derived from the API implementation classes and domain model data structures, with possibly deeply nested structures of elements [Singjai, Zdun, and Zimmermann 2021]. For example, an API implementation might use an Object-Relational Mapper (ORM) to manage the data and might return serializations of instances of the ORM classes in the response messages.   As an API provider, I want to encapsulate my internal data structures so that I can freely change them without breaking backward compatibility of my clients. In domain-driven design terms, I want to keep the integration-level published language separate from the application-level ubiquitous language1.   Stakeholder Concerns      #modifiability, #evolvability, and #information-hiding   API providers want the freedom to change the API implementation without revealing such changes to clients. Such information hiding [Parnas 1972] is crucial for the independent evolvability of API providers and clients.   #developer-experience   API clients want to navigate the required data with minimum effort, taking as few coding steps (expressed as statements and expressions) as possible.   #cohesion, #coupling   API providers strive for low coupling and high cohesion in their endpoint, operation, and message designs.   Initial Position Sketch   The refactoring is eligible for any API operation that uses an implementation type (for example, a domain class or a database-mapped entity) in a request or response message. Note that this implementation type may also appear as a subordinate element within a message structure hierarchy.   For instance, an API provider might return data from a repository directly to the client, as shown in Figure 1. The structure and content of the data are not changed; it is simply passed through.      Figure 1: Introduce Data Transfer Object: Initial Position Sketch. The API provider responds to a client request (1) with a message (2) that contains some data elements.   Design Smells      Leaky encapsulation   Domain layer language constructs (e.g., classes) or abstractions defined in the persistence layer are directly exposed in the API. Such permeable or even completely missing encapsulation of API internal data structures makes an API harder to evolve because it introduces coupling and harms backward compatibility.   Tight coupling of data contract   Anything exposed will be used according to Hyrum’s Law. Hence, leaky encapsulations cause undesired coupling, which may slow development and decrease modifiability and flexibility.   Confetti design   Clients might have to issue many requests to get all the needed data. Fine-grained APIs that rain many small Data Elements on clients are rather flexible but can be tedious to use.   Instructions   To replace an implementation type in a response message with a DTO, perform the following steps:      Create a new data-centric wrapper (e.g., a class in object-oriented languages) that mirrors the attributes of the current message representation. Such Data Transfer Objects (DTOs) [Fowler 2002] are typically implemented as immutable Value Objects [Evans 2003] with structural, value-based equality.   Depending on the implementation framework, add additional serialization logic or mapping configuration information. An example is the JSON-to-Java data binding offered by libraries such as Jackson.   Write unit tests for the DTOs and the mapping logic. If the framework generates the code, such tests might be unnecessary (or already provided by the framework).   Adjust the implementation of the operation to create an instance of the DTO and fill it with the necessary data.   Return the DTO from the operation implementation, adjusting any return types if necessary (so that the new serialization logic can pick them up).   Run the tests to ensure the message structure was not changed accidentally. For example, an integration test might check whether all attributes expected in a JSON object are present.   Include the API change in the API Description if it is visible there; for example, adjust the JSON-Schema part of the OpenAPI description of the API.   Align the sample data in supplemental API documentation artifacts such as tutorials so that it the new structure is featured.   These instructions assume that the DTO is introduced in a response message. If a request message is the target, steps 4 and 5 must be adapted. Instead of creating and returning a DTO, adjust the implementation of the operation to take the DTO as a parameter and convert it back to the API-internal data representation. This refactoring is fully backward-compatible because it only changes the implementation, not the structure of the message.   Target Solution Sketch (Evolution Outline)   When the refactoring has been applied, a mapping step takes place that copies the data to/from the DTO structure. The mapping can preserve the structure or adjust it, depending on the information needs of the message recipient. Figure 2 shows a refactored response message.      Figure 2: Introduce Data Transfer Object: Target Solution Sketch. Instead of passing through the data for the client’s request (1), an additional DTO mapper transforms the data elements before they are returned (2). The implementation types can be changed without affecting the API.   Now that the internal implementation has been decoupled from the response entity, the DTO can transfer additional data, such as Embedded Entities, Link Elements, or Metadata Elements. Such richer messages help against confetti design.   If the internal implementation type evolves, the DTO can implement more complex mapping logic to maintain backward compatibility. Additional metadata, such as a Version Identifier, can also be added to the DTO.   Example(s)   The following excerpt from a Java Spring Boot controller shows an implementation of an operation getMyEntity that fetches an entity from a database repository and directly returns it in its response:   @GetMapping(value = \"/{id}\") public ResponseEntity&lt;MyEntity&gt; getMyEntity(         @ApiParam(value = \"the entity's unique id\")              @PathVariable MyEntityId id) {      MyEntity myEntity = myEntityRepository.getMyEntity(id);     if (myEntity == null) {         throw new ResponseStatusException(             HttpStatus.NOT_FOUND, \"Entity Not Found\");     }     return ResponseEntity.ok(myEntity); }   The Spring GetMapping annotation turns the getMyEntity method into an API operation (HTTP GET) that receives an id parameter and returns a ResponseEntity. The ResponseEntity class has helper methods – such as ok – to generate HTTP messages (200 OK in this case).   The MyEntity class is also used in the object-relational mapping in this example. More specifically, the Java Persistence API (JPA) is used:   @Entity @Table(name = \"my_entities\") public class MyEntity {   String attribute;   ... }   A sample response could look like this:   HTTP/1.1 200 Content-Type: application/json;charset=UTF-8 {   \"attribute\" : \"1c184cf1-a51a-433f-979b-24e8f085a189\" }   When the refactoring has been applied, a MyEntityDto is returned (instead of returning MyEntity directly):   @GetMapping(value = \"/{id}\") public ResponseEntity&lt;MyEntityDto&gt; getMyEntity(         @ApiParam(value = \"the entity's unique id\")              @PathVariable MyEntityId id) {      MyEntity myEntity = myEntityRepository.getMyEntity(id);     if (myEntity == null) {         throw new ResponseStatusException(             HttpStatus.NOT_FOUND, \"Entity Not Found\");     }     MyEntityDto myEntityDto = MyEntityDto.toDto(myEntity);     return ResponseEntity.ok(myEntityDto); }   The MyEntityDto DTO is implemented as follows:   public class MyEntityDto {   // Attributes that mirror those in MyEntity    String attribute;   ...      static MyEntityDto toDto(MyEntity myEntity) {     // Copy attributes from myEntity to new DTO instance   } }   Because the DTO mirrors the attributes of MyEntity, the resulting HTTP response remains unchanged.   Another example comes from the rapid prototyping framework JHipster. The application generator provides the option to generate the Spring Boot code with DTOs. Enabling this option changes the signature of the service class (the + and - stand for added and removed lines, respectively):   -    public Optional&lt;Customer&gt; findOne(Long id) { +    public Optional&lt;CustomerDTO&gt; findOne(Long id) {          log.debug(\"Request to get Customer : {}\", id);          return customerRepository -            .findById(id); +            .findById(id).map(customerMapper::toDto);      }   The CustomerDTO that replaces Customer as the response type in this example is a simple Java bean with attributes, getters, and setters. For the mapping from entity to DTO and vice-versa, JHipster uses MapStruct, an annotation processor that frees the developer from writing trivial mapping code:   import org.mapstruct.Mapper; @Mapper(componentModel = \"spring\", uses = {}) public interface CustomerMapper      extends EntityMapper&lt;CustomerDTO, Customer&gt; {}   Note that the refactoring can also be applied to request messages.   Hints and Pitfalls to Avoid   Do not over-eagerly apply this refactoring to all API operations, but use it only when its value is higher than its cost (note: this is general advice that makes sense in most cases). A good reason might be that the implementation data structures change oftzen and these changes should not be reflected in the structure of the API-level response messages.   DTO classes and mappings are often straightforward to create, so various libraries and code-generation tools exist to automate this task. For example, Lombok is an alternative to MapStruct in the Java ecosystem. The recently introduced Java Records also address this topic.   When using code generation, ensure you know what’s going on behind the scenes and that surprises could be waiting for you (for example, see the third bullet item in How DTOs work in JHipster).   When receiving data, be a Tolerant Reader by making “minimum assumptions about the structure” and only consuming the data you need. This approach has the advantage that recipient code will not be affected if unused parts of the DTO change.   Another motivation for the refactoring can be that additional (meta-)data has to be returned, for example, when applying the Introduce Pagination refactoring.   There is a potential risk of introducing memory leaks in API implementations when developers allocate and release memory manually. Marshaling and unmarshaling of request and response data is often handled by frameworks (for example, JSON to Java and Java to JSON in Jackson when using Spring); caching might occur. Unit tests usually will not catch memory bugs; this requires dedicated reliability tests.   DTOs are meant for communicating with external clients and should not be used internally in the API implementation. If the API implementation needs to pass around data internally, it should use the existing data structures directly. See the article about Internal Data Transfer Objects by Phil Calçado for reasons why DTOs should not be used internally.   Related Content   The DTOs and related mapping logic are usually placed in an implementation-level Service Layer [Fowler 2002].   Chapter 8, “Evolve APIs”, in [Zimmermann et al. 2022] discusses evolution strategies for APIs. Refactorings such as Introduce Version Identifier, Introduce Version Mediator, Relax Evolution Strategy, and Tighten Evolution Strategy provide guidance on refactoring an API towards those patterns.   Domain Driven Design (DDD) offers additional techniques and patterns to structure domain classes. A brief introduction to Tactic DDD can be found in the Design Practice Repository (DPR) on GitHub and the corresponding DPR eBook [Zimmermann and Stocker 2021].   Many Enterprise Integration Patterns [Hohpe and Woolf 2003] are related. For instance, Content Enricher and Content Filter can be used to wrap and map implementation-internal data.   Step 4 of the Stepwise Service Design in DPR [Zimmermann and Stocker 2021] advises to “foresee a Remote Facade that exposes Data Transfer Objects (DTOs) in the request and response messages of its API operations to decouple the (published) languages of frontends and backends and to optimize the message exchange over the network regarding exchange frequency and message size.” The Remote Facade that is mentioned in the quote helps to “translate coarse-grained methods onto the underlying fine-grained objects.” This means that the DTO can be used to restructure the data so that clients can easily interact with it while using the network efficiently.   Martin Fowler describes the code-level refactoring Introduce Parameter Object in “Refactoring – Improving the Design of Existing Code”[Fowler 2018]. The Refactoring.Guru website features this refactoring as “Introduce Parameter Object”.   References             Evans, Eric. 2003. Domain-Driven Design: Tacking Complexity in the Heart of Software. Addison-Wesley.               Fowler, Martin. 2002. Patterns of Enterprise Application Architecture. Boston, MA, USA: Addison-Wesley Longman Publishing Co., Inc.               ———. 2018. Refactoring: Improving the Design of Existing Code. 2nd ed. Addison-Wesley Signature Series (Fowler). Boston, MA: Addison-Wesley.               Hohpe, Gregor, and Bobby Woolf. 2003. Enterprise Integration Patterns: Designing, Building, and Deploying Messaging Solutions. Addison-Wesley.               Parnas, D. L. 1972. “On the Criteria to Be Used in Decomposing Systems into Modules.” Commun. ACM 15 (12): 1053–58. https://doi.org/10.1145/361598.361623.               Singjai, Apitchaka, Uwe Zdun, and Olaf Zimmermann. 2021. “Practitioner Views on the Interrelation of Microservice APIs and Domain-Driven Design: A Grey Literature Study Based on Grounded Theory.” In 18th IEEE International Conference on Software Architecture (ICSA 2021). https://doi.org/https://doi.org/10.5281/zenodo.4493865.               Zimmermann, Olaf, and Mirko Stocker. 2021. Design Practice Reference - Guides and Templates to Craft Quality Software in Style. LeanPub. https://leanpub.com/dpr.               Zimmermann, Olaf, Mirko Stocker, Daniel Lübke, Uwe Zdun, and Cesare Pautasso. 2022. Patterns for API Design: Simplifying Integration with Loosely Coupled Message Exchanges. Addison-Wesley Signature Series (Vernon). Addison-Wesley Professional.                        Ubiquitous Language is one of the core patterns in Domain Driven Design [Evans 2003]. &#8617;           ","categories": [],
        "tags": [],
        "url": "/refactorings/introducedatatransferobject",
        "teaser": null
      },{
        "title": "Introduce Pagination",
        "excerpt":"also known as: Paginate Responses, Slice Response Message   Context and Motivation   An API operation returns a large sequence of Data Elements. For example, such a sequence may enumerate posts in a social media site or list products in an e-commerce shop. The API clients are interested in all Data Elements in the sequence but have reported that processing a large amount of data at once is challenging for them.   As the API provider, I want to return data sets in manageable chunks so that clients are not overwhelmed by a huge amount of data arriving at once.   Stakeholder Concerns      #performance, #resource-utilization   Transferring all Data Elements at once can lead to huge response messages that burden receiving clients and the underlying infrastructure (i.e., network and application frameworks as well as databases) with a high workload. For instance, single-page applications that receive several megabytes of JSON might freeze until all contained JSON objects have been decoded.   &lt;!-- --&gt;      #data-access-characteristics   In principle, the client wants to access all data elements, but not all have to be received at once or every time. For example, older posts to a social media site might be less relevant than recent ones and can be retrieved separately.   Initial Position Sketch   The API provider currently returns an extensive sequence of Data Elements in the response messages of the operation. Figure 1 shows this initial position sketch.      Figure 1: Introduce Pagination: Initial Position Sketch. The client’s request (1) is met by a large sequence of data elements (2).   This refactoring targets an API Retrieval Operation and its request and response messages.   Design Smells      High latency/poor response time   Responses take a long time to arrive at the client because a lot of data has to be assembled and transmitted. This might be evident in a provider-side log file analysis or client-side performance metrics.   Overfetching   A client may not need all data (at once or at all) and truncate an overly large dataset. Since this truncation happens on the client side, data was unnecessarily processed and transmitted.   Spike load   Regular requests for large amounts of data cause Periodic Workload [Fehling et al. 2014] for CPU and memory, for instance, when a large JSON object has to be constructed (on the provider side) and read (on the client side). For example, the “Time-Bound Report” variant of a Retrieval Operation might lead to relatively large responses, depending on the time interval size chosen.   Instructions   Decide on a variant of Pagination that best fits your API: Page-Based, Offset-Based, Cursor-Based, or Time-Based Pagination. Clients request the data differently in these variants; see the Pagination pattern description [Zimmermann et al. 2022] for details on the variants and their pros and cons.      All variants involve certain metadata, so if the current response message directly returns the underlying domain model elements, possibly contained in a list, wrap the structure in a Data Transfer Object (DTO) first by applying the Introduce Data Transfer Object refactoring.   Add additional response attributes to the DTO to hold the metadata required for Pagination (for instance, page size, page number, and the total number of pages for the Page-Based pattern variant).   Adjust the expected parameters in the request message to give the client control over the number of results returned. Provide default values so that existing clients will continue to work.   Enhance the unit and integration tests to include and check for these additional attributes. Test with different chunk sizes. Include complete versus partial retrievals and changes to data while being paginated to the test suite.   Update API Description, sample code, tutorials, etc., with information about the Pagination options (for instance, variant, metadata syntax, semantics, and session management concerns).   Increase the version number as suggested under Semantic Versioning. The refactoring typically results in a major update, but a minor update might be sufficient if the API provider implements the change in a backward-compatible way.   When already following the API best practice of consistently returning an object as a top-level data structure, it is straightforward to implement Pagination in a backward-compatible manner, returning all results as a single page if no control metadata appears in incoming requests. While this approach is backward-compatible, it does not remove any of the above smells.   Target Solution Sketch (Evolution Outline)   After the refactoring, the client indicates the desired amount and position in the sequence of data in their request messages (depending on the Pagination variant). In Figure 2, the number of elements, offset (desired first element, that is), and so on is represented by Metadata Elements.      Figure 2: Introduce Pagination: Target Solution Sketch. After the refactoring, the client includes metadata in the request (1) that tells the provider which elements to return. In its response (2), the provider returns the desired elements, along with more metadata (for instance, the total number of elements available). This exchange can then be repeated with follow-up requests (3) and responses (4), where the client can specify the next page (or offset, cursor, depending on the chosen pattern variant).   More but smaller messages are exchanged after the refactoring has been applied.   Example(s)   In this example, we will add Offset-Based Pagination to the Customer Core service of the Lakeside Mutual sample application. The customers endpoint in this service returns a list of customer representations:   $ curl http://localhost/customers [ {   \"customerId\" : \"bunlo9vk5f\",   \"firstname\" : \"Ado\",   \"lastname\" : \"Kinnett\",   ... }, {   \"customerId\" : \"bd91pwfepl\",   \"firstname\" : \"Bel\",   \"lastname\" : \"Pifford\",   ... } ]   Note that the response is a JSON array of objects. To transmit the Pagination metadata, we first wrap the response in a JSON object (this wrapping is usually done by introducing a DTO that encapsulates the Data Elements), with a customers property to hold the entities:   $ curl http://localhost/customers {   \"customers\" : [ {     \"customerId\" : \"bunlo9vk5f\",     \"firstname\" : \"Ado\",     \"lastname\" : \"Kinnett\",     ...   }, {     \"customerId\" : \"bd91pwfepl\",     \"firstname\" : \"Bel\",     \"lastname\" : \"Pifford\",     ...   } ] }   Unfortunately, this makes the response backward incompatible. Initially, the array was returned at the top level of the response, but now it is nested inside a customers object. Enabling such future extensibility is why API guidelines (e.g., from Zalando) recommend always returning an object as the top-level data structure in the first place.   With the basic structure in place, we can now add HTTP query parameters (limit, offset) and return the Pagination metadata (limit, offset, size) in our response. Here is a request for the next chunk of elements (including the JSON response to it):   $ curl http://localhost/customers?limit=2&amp;offset=2 {   \"limit\" : 2,   \"offset\" : 2,   \"size\" : 50,   \"customers\" : [ {     \"customerId\" : \"qpa66qpilt\",     \"firstname\" : \"Devlin\",     \"lastname\" : \"Daly\",     ...   }, {     \"customerId\" : \"en2fzxutxm\",     \"firstname\" : \"Dietrich\",     \"lastname\" : \"Cordes\",     ...   } ] }   See the Lakeside Mutual repository for the full Spring Boot implementation, including HATEOAS links and filtering.   Hints and Pitfalls to Avoid   The Data Elements the operation returns typically have an identical structure, as in our example above. Still, Pagination can also be used if the structures of the individual Data Elements differ from each other, as long as there is a sequence of elements that can be split up. If the structure of the response does not comprise a sequence of elements that can be split into pages, the Extract Information Holder refactoring offers an alternative solution to reduce the amount of data transferred.   The API implementation should ensure that the order of elements is consistent when implementing Pagination. For example, the API provider can specify an explicit order by when querying a relational database. Otherwise, clients might receive inconsistent or duplicate results across multiple pages.   Keep in mind that not all API clients are part of end user applications. Backend services can also be API clients that may want to paginate the data they receive.   If the API deployment infrastructure involves load balancers and failover/standby configurations, keep the following in mind:1      The request for a follow-up page (Step 3 of Figure 2) could go to a different API service provider instance than the first initial request. In that case, that (second) instance would perform another database request to retrieve the second page. However, the data on that second page could have changed in the repository between the two page requests. So this only works for static data that does not change often.   Data consistency/transaction mechanism: Assuming we are dealing with highly dynamic repository data (e.g., the backend database is constantly changing), we need to either make sure that all page requests reach the same service instance that initially retrieved the data from the database (effectively making the service stateful), or develop a caching mechanism in the repository so that data changes between page requests are not causing data inconsistencies in the client.   If the service instance fails between the two page requests (assuming the service is now stateful, and we have a routing rule to reach the same instance with each page request), the provider has to notify the client that Pagination has failed entirely, and the client then must retrieve the first page again.   Instead of adding Pagination metadata to the body of the response message, it can be transmitted in HTTP headers, as in the GitHub API. This can be an alternative implementation approach if the body of the response message cannot be adjusted for backward compatibility.   Related Content   The Introduce Data Transfer Object refactoring prepares request and response messages to introduce the Pagination metadata.   “Patterns for API Design” [Zimmermann et al. 2022] describes Pagination and its variants in detail and points at additional information.   References             Fehling, Christoph, Frank Leymann, Ralph Retter, Walter Schupeck, and Peter Arbitter. 2014. Cloud Computing Patterns: Fundamentals to Design, Build, and Manage Cloud Applications. Springer. https://doi.org/10.1007/978-3-7091-1568-8.               Zimmermann, Olaf, Mirko Stocker, Daniel Lübke, Uwe Zdun, and Cesare Pautasso. 2022. Patterns for API Design: Simplifying Integration with Loosely Coupled Message Exchanges. Addison-Wesley Signature Series (Vernon). Addison-Wesley Professional.                        Thanks to Andrei Furda for suggesting this advice. &#8617;           ","categories": [],
        "tags": [],
        "url": "/refactorings/introducepagination",
        "teaser": null
      },{
        "title": "Introduce Version Identifier",
        "excerpt":"also known as: Apply Semantic API Versioning, Make Version Explicit (in Messages), Provide Compatibility Metadata   Context and Motivation   An API has been deployed to a production environment and is used by clients. The provider is evolving the API with new or improved functionality. Existing clients might have to be adjusted when a new version is released.   As an API provider, I want to indicate versions and their compatibility properties explicitly at design time and runtime so that clients can react accordingly to changes that affect them during API evolution.   Stakeholder Concerns      #maintainability   There are many reasons to change an API (besides quality refactorings [Stocker and Zimmermann 2021]). It should be changed as much as required and as little as possible, and ripple effects be avoided. One of the first steps in related maintenance tasks is determining which system parts should be changed. The impact of the major and minor changes on other parts should be kept at a minimum, but it is worth communicating when such changes occur.   #compatibility   Explicit versions, possibly introducing breaking changes, might appear costly and anti-agile or not RESTful at first glance. However, fixing bugs caused by not knowing about versioning mismatches is often expensive.   #developer-experience   Since Version Identifiers can be placed in protocol headers (in most protocols) or in the message payload, additional learning and decision making is required. Accessing protocol headers differs from accessing payload in terms of code to be written, tool and library support, and portability.   Initial Position Sketch   The entire API or individual parts, such as endpoints, operations, or message parts can be versioned. Here, we primarily target endpoint versioning.   All clients invoke operations exposed by a single, unversioned endpoint. Figure 1 shows this rather simple Initial Position Sketch.      Figure 1: Introduce Version Identifier: Initial Position Sketch. An API provider has deployed an API with a single endpoint that clients use.   Design Smells      Tacit semantic changes up to incompatibilities creep in   While the technical API contract remains unchanged, the meaning of the received or returned data might change over time. Such semantic mismatches between older and newer versions should be documented in the API Description and examples and then caught during testing, ideally in an automated fashion. Implicit versioning an applying the Tolerant Reader pattern [Daigneau 2011] might hide such changes and their impact for quite some time.   Resistance to change caused by uncertainty   API providers may hesitate to implement necessary changes due to a lack of clarity in their strategy for evolving the API. Clients might be reluctant to upgrade to new versions because they are unable to assess the imposed changes on their side.   See “Interface Evolution Patterns — Balancing Compatibility and Extensibility across Service Life Cycles” [Lübke et al. 2019] for other smells related to API versioning and countermeasures.   Instructions   The introduction and continued use of explicit an Version Identifier has to be planned, executed, and sustained:      Decide on scope and naming conventions for the Version Identifier, for instance, following the Semantic Versioning specification when assigning and communicating version numbers (currently standing at Version 2.0.0).1   Define an evolution roadmap, selecting one or more lifecycle management strategies to define the lifetime of the version; see the related refactorings Tighten Evolution Strategy and Relax Evolution Strategy.   Decide where to place the Version Identifier for each API element to be versioned. For instance, possible locations are endpoint address, message payload, and protocol header.   Put the version-enhanced endpoint addresses in the API implementation code or update the message construction code, depending on where the Version Identifier has been added.   Update the API documentation with the new Version Identifier(s) and meta-information about the meaning of this version information (for instance, consequences of certain version numbers regarding compatibility).   The pattern description of Version Identifier provides more information about versioning scopes, identifier placement (location), and compatibility considerations [Zimmermann et al. 2022].   Target Solution Sketch (Evolution Outline)   Once a Version Identifier has been introduced, clients can choose which version of an endpoint they want to work with (assuming that multiple versions are supported, as described in the Two in Production pattern). Figure 2 illustrates this new, more flexible setup.      Figure 2: Introduce Version Identifier: Target Solution Sketch. The API provider has introduced a Version Identifier to the API, allowing clients to choose which version of an endpoint they want to work with.   Example(s)   Depending on the chosen location of the Version Identifier, clients enact their usage decision in the message payload or a header (see section “Instructions” above). In HTTP, it may be part of the endpoint address (relative URI path):   GET /customers/1234 Accept: text/json+customer; version=1.0 ...   or   GET /v2/customers/1234 ...   The API Stylebook compiled by “API Handyman” Arnaud Lauret points at many additional examples in its design topic “Updates and Versioning”.   Hints and Pitfalls to Avoid   Before and when applying this refactoring, make sure to:      Involve API clients in the decisions about and planning of API evolution (assuming that they are known and willing to participate).   Stay backward-compatible whenever possible, but do not hesitate to upgrade the major version when necessary.   Provide migration aids such as change logs, code snippets, and mappings of identifiers and parameters from old to new versions.   Be aware of design challenges caused by automatic routing. For example, Version Identifiers in encrypted message payloads might not be visible to intermediaries and, therefore, can not be used for routing purposes.   “Interface Evolution Patterns — Balancing Compatibility and Extensibility across Service Life Cycles” provides further hints. For example, “stick to a standardized and consistent versioning strategy, e.g., decide which objects to version consistently (operations, data types, etc.) or which versioning schema to use (e.g., Semantic Versioning)” [Lübke et al. 2019].   Related Content   Other refactorings dealing with API evolution are Introduce Version Mediator, Tighten Evolution Strategy, and Relax Evolution Strategy.   The evolution patterns in Zimmermann et al. [2022] cover versioning. For instance, there is a pattern called Semantic Versioning. The concept of Service Level Agreement (SLA) is captured in pattern form as well; an SLA pertains to a single or multiple versions and should specify and reference those versions explicitly.   Arnaud Lauret’s book on Web API design covers the topic [Lauret 2019], and the blog post “5 Ways to Version APIs “ also discusses options with pros and cons.   James Higginbotham provides advice regarding “When and How Do You Version Your API?.   References             Daigneau, Robert. 2011. Service Design Patterns: Fundamental Design Solutions for SOAP/WSDL and RESTful Web Services. Addison-Wesley.               Lauret, Arnaud. 2019. The Design of Web APIs. Manning.               Lübke, Daniel, Olaf Zimmermann, Cesare Pautasso, Uwe Zdun, and Mirko Stocker. 2019. “Interface Evolution Patterns: Balancing Compatibility and Extensibility Across Service Life Cycles.” In Proceedings of the 24th European Conference on Pattern Languages of Programs. EuroPLop ’19. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3361149.3361164.               Stocker, Mirko, and Olaf Zimmermann. 2021. “From Code Refactoring to API Refactoring: Agile Service Design and Evolution.” In Service-Oriented Computing, edited by Johanna Barzen, 174–93. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-87568-8_11.               Zimmermann, Olaf, Mirko Stocker, Daniel Lübke, Uwe Zdun, and Cesare Pautasso. 2022. Patterns for API Design: Simplifying Integration with Loosely Coupled Message Exchanges. Addison-Wesley Signature Series (Vernon). Addison-Wesley Professional.                        Note that the Semantic Versioning specification has a Version Identifier and applies Semantic Versioning itself. &#8617;           ","categories": [],
        "tags": [],
        "url": "/refactorings/introduceversionidentifier",
        "teaser": null
      },{
        "title": "Introduce Version Mediator",
        "excerpt":"also known as: Add Compatibility Gateway, Add Tolerant Proxy, Support Virtual Version   Context and Motivation   An API runs in production. One of its supported versions will be retired soon, but existing clients still use it. One or more breaking changes have been introduced in subsequent, active versions.1   As an API client, I want to continue to call a deprecated API for some time, and I expect the provider to support me with a temporary solution for doing so. The behavior of this solution should be identical to those of the API that I have been using so far.   We will call the API version that will go out of service the “old” API and its successor “new” API.   Stakeholder Concerns      #flexibility and #evolvability   To evolve an API, providers must have the flexibility to refactor, redesign, and adapt an API over time. Ideally, this happens without breaking compatibility, but this is not always realistic.   #developer-experience   Breaking changes in APIs and pressure to upgrade may frustrate client developers, especially if they must migrate to a newer version on short notice. For instance, cloud application developers sometimes have to react rather quickly to changes introduced by their public providers.   #maintainability   Fewer components and/or code paths that handle deprecated behavior make an API and its implementation easier to maintain for the provider.   See “Interface Evolution Patterns — Balancing Compatibility and Extensibility across Service Life Cycles” [Lübke et al. 2019] for a general discussion of desired qualities, their conflicts, and related trade-offs.   Initial Position Sketch   This architectural refactoring affects the following API elements:      An endpoint and at least one of its operations (with their roles and responsibilities)   Representation elements in request and response messages of these operations with their names, roles, and types (including information about value ranges, optionality, and cardinality)   API clients and the remote communication proxies they use   See Figure 1 for a visualization of the initial position.      Figure 1: Introduce Version Mediator: Initial Position Sketch. Client communicates with Version 1 of an API Endpoint   Design Smells      Evolution strategy does not meet client expectations   The API provider has decided to commit a short lifetime of an API version only, or has announced to retire one or more active versions soon. This has caused a negative reaction in the API client community (the related refactorings Tighten Evolution Strategy and Relax Evolution Strategy explain this smell further).   Resistance to change caused by uncertainty   One or more breaking change of the API have happened, or the lifetime guarantee has been softened.2 However, clients are unwilling or unable to migrate to the latest version immediately. They might fear the effort and risk of the migration or they might lack confidence and trust in the new version.   Large and/or partially unknown user base   API providers are not in control of their users and lack information about them. The less information and control a provider has, the higher the risk of impacting clients negatively (or losing them) when making breaking changes in upgrades.   Instructions   Add a new endpoint to the API for the “new” API version. Derive its contract from the “old” one and adjust the “old” endpoint to mediate “old” operations and their representation elements to “new” ones with mapping rules.   When establishing the mapping rules for the operations whose “old” and “new” versions are incompatible, start with the request messages:      Identify and mark the fields that remain unchanged and do not require a mapping.   Define a mapping rule for fields that are renamed only and can be mapped one-to-one (pass-through).   Find the fields that change their type, and define a mapping rule to implement the type change:            If a previously optional field becomes mandatory, define a mapping rule that leverages a Content Enricher a.k.a. Data Enricher to define a default value (filler). No mapping action is required for the opposite case (mandatory fields becoming optional).       Mark the fields that change cardinality, for instance from atomic to list/set (and vice versa).       Do the same for other basic type changes, such as from numeric to strings.           Find the fields that are added in the new version. Add a Content Enricher to support “old” clients. Define a default value or make the newcomer optional (to preserve compatibility).   Mark the fields that disappear in the new version. Add a Content Filter to mediate requests from “old” clients (and log the fact that some data is no longer processed, having double-checked that this makes sense).   Find places where two (or more) “old” fields map to one “new” field. For each such place, define a mapping rule realizing an Aggregation strategy. Define a Splitter rule for the opposite case, one “old” field mapping to two or more “new” fields. These two cases can also occur in combination (which actually can be seen as the default/catch case); a Scatter-Gather rule can handle them.   Continue with the response messages and define similar rules for them, including error cases. If a representation element in a response used to be a single Atomic Parameter but now is set- or list-valued, a Content Filter can select a single element to return. However, the “old” client and the mediator might suffer from information loss through this filtering. Additional patterns such as Context Representation might be able to provide additional (meta-)information in such situations.3   Include any custom request and response headers in the mapping for requests and responses.   Secure the mediator endpoint exactly as the updated main endpoint.   Target Solution Sketch (Evolution Outline)   A rule-based Compatibility Mediator implements the compatibility mappings, either as plain code or declaratively, and acts as a gateway between “old” clients and the “new” provider. Figure 2 shows this solution.      Figure 2: Introduce Version Mediator: Target Solution Sketch. Client communication is mapped to a new endpoint via a Compatibility Mediator.   The mediator should be a transitional, interim solution preserving a good client developer experience and giving clients more time to adopt the “new” API.   Example(s)   The fictitious insurance firm Lakeside Mutual could expose the following Customer Core microservice (notation: MDSL):   API description LakesideMutual version \"v1.0.0\"  data type CustomerDTO1 {\"name\":D&lt;string&gt;}  endpoint type CustomerCoreOriginalContract exposes    operation createCustomerMasterData     expecting payload \"customerData\": CustomerDTO1     delivering payload \"customerId\": ID&lt;int&gt;   operation readCustomerMasterData      expecting payload \"customerId\": ID&lt;int&gt;     delivering payload \"customerData\": CustomerDTO1  API provider LakesideMutualAPI   offers CustomerCoreOriginalContract   at endpoint location \"http://some.original.address\"   via protocol HTTP binding resource Home  API client CustomerRelationshipManagementApplication   consumes CustomerCoreOriginalContract   via protocol HTTP    Lakeside Mutual could then update its Customer Core interface (for instance, after a merger with another company):   API description LakesideMutual version \"v2.0.0\"  data type CustomerDTO1 {   \"name\":D&lt;string&gt;, \"zipString\":D&lt;string&gt;,    \"toBeSunset\":MD&lt;bool&gt;}  data type CustomerDTO2 {   \"firstName\":D&lt;string&gt;, \"lastName\":D&lt;string&gt;,    \"zipCode\": D&lt;int&gt;, \"newKey\":ID&lt;int&gt;}  // not featuring all deviations here (as defined  // in Steps 1 to 6 in \"Instructions\")      endpoint type CustomerCoreRevisedContract exposes    operation createCustomerMasterData     expecting payload \"customerData\": CustomerDTO2     delivering payload \"customerId\": ID&lt;int&gt;   operation readCustomerMasterData      expecting payload \"customerId\": ID&lt;int&gt;     delivering payload \"customerData\": CustomerDTO2     API client CustomerRelationshipManagementApplication   // will no longer work:   consumes CustomerCoreOriginalContract   via protocol HTTP    Support for the “old” contract can be modeled as a mediation gateway in MDSL (which does not imply that a visible API gateway is deployed; the mediation can happen in the background):   API gateway Version1ToVersion2Mediator   offers CustomerCoreOriginalContract     at endpoint location \"http://some.new.address\"     via protocol HTTP binding resource Home   consumes CustomerCoreRevisedContract      from LakesideMutualAPI     via protocol HTTP   mediates from CustomerDTO1 to CustomerDTO2      element zipString to zipCode     // not featuring element-level mapping rules here   Hints and Pitfalls to Avoid   The user story and the smells motivating this refactoring mention scenarios in which it may be applied; it is also important to know when not to use a particular design.   To decide when not to apply this refactoring, analyze whether the roles of an endpoint, responsibilities of an operation, and/or semantics of a Data Element change. Such changes require more than a rule-based mediator; in such situations, this refactoring is less suited.   As a general rule for any communication party (API client and provider), apply Postel’s Law and be liberal when consuming messages and conservative when producing them.   Having decided to apply this refactoring, make sure to:      Catch and handle mapping errors, both at specification time and at runtime.   Test all combinations of the “old” versus “new” clients and provider that appear in the refactored system landscape; up to four (two times two) cases might occur. Add test data for all steps/situations from the step descriptions further up that may occur. Include mapping errors in the tests (for instance, set-valued responses that the introduced Content Filters realizing the Version Mediator are not prepared to process).   Monitor the performance of the Compatibility Mediator (in particular, when it is realized as a mapping rule engine) and end-to-end latency (as the number of request/response messages is doubled).   Do not prolong the lifetime of the intermediary/the temporary mediation endpoint; for instance, do not place a second gateway in front of the gateway to cope with a future change of a different kind.   A domain-specific language, either embedded in a general-purpose language or explicit, might be an appropriate choice for expressing the mapping rules. Many application integration tools provide such languages (often proprietary). MDSL is a technology-independent contract language that supports mediation rules.   An API Gateway may play the role of a Version Mediator. An example of such gateway usage can be found in the blog post “8 Common API Gateway Request Transformation Policies”. LinkedIn also uses an API Gateway in their new Marketing APIs that support request mapping to mediate between API versions.   Related Content   An application of Tighten Evolution Strategy may trigger this refactoring. And Introduce Version Identifier might have to be applied before this one so that clients can learn about versions and their (in-)compatibilities.   The Enterprise Integration Patterns category Message Transformation provides partial solutions; the patterns Content Enricher and Content Filter are used to realize the Compatibility Mediator (which effectively is a special-purpose Content-Based Router).   For an example of Enterprise Service Bus product capabilities and integration services, refer to Scenario 4/Figure 7 in “Enterprise Service Bus” by Jürgen Kress et al. This technical article, available on a vendor site, positions the pattern, presents usage scenarios, and suggests selection criteria.   In object-oriented programming, the Adapter design pattern [Gamma et al. 1995] provides a different view on the interface of a class so that it can be used by clients that cannot work with the original interface easily. Note that there is also a Mediator pattern, whose goal is different (decouple objects from each other).   Also related are:      Data mapping and Enterprise Application Integration (EAI) tools such as Apache Camel, possibly complemented with a data mapper such as Nomin as featured in “An integration job engine for Apache Camel” as well as expression languages such as the Spring Expression Language (SPeL) or XPath and its JSON pendants   An API Versioning/Evolution DSL described in “Continuous API Evolution in Heterogeneous Enterprise Software Systems” [Knoche and Hasselbring 2021]   Extract-Transform-Load (ETL) tools in the Data Warehouse and Information Management communities   Smart proxies in service middleware operating on change-aware contracts are emerging [Knoche and Hasselbring 2021].   References             Gamma, Erich, Richard Helm, Ralph Johnson, and John Vlissides. 1995. Design Patterns: Elements of Reusable Object-Oriented Software. Addison-Wesley.               Knoche, Holger, and Wilhelm Hasselbring. 2021. “Continuous API Evolution in Heterogenous Enterprise Software Systems.” In 18th IEEE International Conference on Software Architecture, ICSA 2021, Stuttgart, Germany, March 22-26, 2021, 58–68. IEEE. https://doi.org/10.1109/ICSA51549.2021.00014.               Lübke, Daniel, Olaf Zimmermann, Cesare Pautasso, Uwe Zdun, and Mirko Stocker. 2019. “Interface Evolution Patterns: Balancing Compatibility and Extensibility Across Service Life Cycles.” In Proceedings of the 24th European Conference on Pattern Languages of Programs. EuroPLop ’19. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3361149.3361164.                        which should, but cannot always be avoided; sometimes, it is better to break an existing client and cause work for its developers/maintainers rather than pretending that nothing has changed and letting some hard-to-catch bugs creep in &#8617;                  while this should generally be avoided, this is not always possible; the provider might have good reasons to do it &#8617;                  Such cases should be avoided if at all possible, for instance, by providing the “old” and the “new” version of the operation in parallel in the “new” API. &#8617;           ","categories": [],
        "tags": [],
        "url": "/refactorings/introduceversionmediator",
        "teaser": null
      },{
        "title": "Make Request Conditional",
        "excerpt":"also known as: Introduce Request Conditions, Cache Responses   Context and Motivation   An API endpoint provides data that changes rarely, and thus, some clients request and receive the same data frequently. Preparing and retransmitting data already available to the clients is unnecessary and wasteful.   As an API provider, I want to be able to tell clients that they already have the most recent version of certain data so that I do not have to send this data again.   Stakeholder Concerns      #performance, #green-software   Response, throughput, and processing times concern API clients and providers. Unused data that is prepared, transported, and processed wastes resources, which should be avoided.   #data-access-characteristics   API clients might use caching and do not want to retrieve data they already have.   #developer-experience, #simplicity   Knowing when and how long to cache which data might be challenging for API clients and providers. Permanent or temporary storage is required. These valid concerns have to be balanced with the desire for performance.   Initial Position Sketch   Figure 1 shows the initial position sketch for this refactoring. The client requests some data from the API. Later, the client wants to ensure that the data is still up to date and sends a second request for the same data.      Figure 1: Make Request Conditional: Initial Solution Sketch: In the first message exchange (1–2), the endpoint returns one or more Data Elements. Later on (3), the client requests the data from the endpoint again. Because nothing has changed, the provider returns the same data (4) as in the previous response.   This refactoring targets a single API operation and its request and response messages.   Design Smells      High latency/poor response time   Load on the API provider is unnecessarily high because the same data is processed and transferred many times over.   Spike load   Regular requests for large amounts of data can cause Periodic Workload or Unpredictable Workload [Fehling et al. 2014] for CPU and memory, for instance, when a relatively large JSON object representing the requested data has to be constructed (on the provider side) and read (on the client side).   Polling proliferation   Clients that participate in long-running conversations and API call orchestrations ping the server for the current status of processing (“are you done?”). They do so more often than the provider-side state advances.   Instructions   Instead of transmitting the same data repeatedly, the request can be conditional. Condition information is exchanged as metadata to allow the communication participants to determine whether the client already has the latest data version.      Decide for one of the two variants of the Conditional Request pattern: data can a) be timestamped or b) responses be fingerprinted (by calculating a hash code of the response body) [Zimmermann et al. 2022].   Adjust the API specification and implementation to include a conditional Metadata Element in both request and response messages. The request metadata should be optional so that it can be omitted in initial requests; optionality also brings backward compatibility. For the response message, check if the transport protocol provides a special status for this case and consider using it (such as HTTP status code 304 Not Modified).   In the API implementation, evaluate the condition – for example, by comparing the previously mentioned timestamps or fingerprints/hashes – and respond with an appropriate message.   Create additional unit or integration tests for the API implementation that validate combinations of metadata presence or absence (with changed and unchanged data).   If several operations in the API use Conditional Requests, investigate whether your framework offers a way to implement this functionality in a generic way.   Adjust the API client implementations that you oversee (for instance, API usage examples) to utilize the new feature: send conditions and keep previously received data. Adjust the API tests as well.   Document the changes, for example, in a changelog data release notes, and release a new API version.   This refactoring can be applied incrementally, for instance, to a single operation or a group of operations. Backwards compatibility is preserved by making the condition metadata optional in the request.   Target Solution Sketch (Evolution Outline)   Comparing the solution in Figure 2 to the initial position sketch, we see that follow-up requests return a special response message indicating that the data has not changed. The client can then continue to use the data it has already received.      Figure 2: Make Request Conditional: Target Solution Sketch: The first exchange (1–2) is the same as in the initial position. In the second request though, the client includes the condition metadata (3) in its request, which in turn allows the provider to respond with a special “not modified” message (4) if the data has not changed.   Example(s)   The Customer Core microservice of the Lakeside Mutual sample application implements conditional requests in its WebConfiguration class. Classes annotated with @Configuration can be used to customize the configuration of the Spring MVC framework. The fingerprint-based variant of Conditional Request is applied in its request and response messages:   @Configuration public class WebConfiguration implements WebMvcConfigurer {     ...      /**      * This is a filter that generates an ETag value based      * on the content of the response. This ETag is compared      * to the If-None-Match header of the request. If these      * headers are equal, the response content is not sent,      * but rather a 304 \"Not Modified\" status instead.      *       * By marking the method as @Bean, Spring can call this      * method and inject the dependency into other components,       * following the inversion of control principle.      * */     @Bean     public Filter shallowETagHeaderFilter() {         return new ShallowEtagHeaderFilter();     } }   The ShallowEtagHeaderFilter class is already included in the Spring Framework. Because it is implemented as a filter applied to all requests and responses, the implementation of the individual operations does not have to be adjusted. A consequence of this implementation, and the reason why it is called “shallow” ETag, is that responses are still assembled, hashed and replaced with a 304 Not Modified response if the hash matches the ETag header.   Alternatively, a Version Identifier could be introduced in the (meta)data to avoid having to retrieve and hash the entire data. This is also supported by Spring Data REST for classes that have an @Version property:   @Entity public class CustomerAggregateRoot implements RootEntity {      @Version     Long version;       @EmbeddedId     private CustomerId id;      ... }   Hints and Pitfalls to Avoid   Before and when making requests conditional, ask yourself:      How does the additional overhead to calculate the hashes, or the extra storage used by timestamps and versioning numbers compare to the expected savings?   Does the condition cover all the data returned in the response? For example, when the data contains nested structures, a change in a contained element must be detected. Otherwise, clients might work with stale data.   How does a Conditional Request count towards a Rate Limit [Zimmermann et al. 2022]?   Be careful when combining Conditional Requests with a Wish List or Wish Template. The data might not have changed, but the client could request different parts of it. In this case, the cached data is unlikely to be sufficient.   Do not mindlessly start caching all API responses on the client side. Cache design is hard to get right. For instance, knowing when to invalidate cache entries is not trivial [Karlton 2009].   The Conditional Request pattern and this refactoring assume that the server is responsible for evaluating the condition. However, it may make sense for the client to evaluate the condition in order to avoid sending a request to the server. For example, a client could consult the HTTP Expires header to decide whether the data retrieved from the server is still current [Fielding, Nottingham, and Reschke 2022]. This doesn’t guarantee that the client has the latest data, but depending on the use case, that may not be a problem.   Related Content   The online presentation of the Conditional Request pattern coverage presents an example leveraging the Spring framework.   An operation that returns nested data holders that change more or less often than the containing data can prevent this refactoring from being applied. In that case, applying the Extract Information Holder refactoring first to separate the nested data holders from the containing data can help. Chapter 7 of Zimmermann et al. [2022] provides a comprehensive introduction to API quality.   Our catalog includes an Introduce Version Identifier refactoring that focuses on versioning endpoints, not Data Elements.   Conditional requests in Hypertext Transfer Protocol (HTTP/1.1) are defined by RFC 7232 [Fielding and Reschke 2014].   References             Fehling, Christoph, Frank Leymann, Ralph Retter, Walter Schupeck, and Peter Arbitter. 2014. Cloud Computing Patterns: Fundamentals to Design, Build, and Manage Cloud Applications. Springer. https://doi.org/10.1007/978-3-7091-1568-8.               Fielding, Roy T., Mark Nottingham, and Julian Reschke. 2022. “HTTP Caching.” Request for Comments. RFC 9111; RFC Editor. https://doi.org/10.17487/RFC9111.               Fielding, Roy T., and Julian Reschke. 2014. “Hypertext Transfer Protocol (HTTP/1.1): Conditional Requests.” Request for Comments. RFC 7232; RFC Editor. https://doi.org/10.17487/RFC7232.               Karlton, Phil. 2009. “Two Hard Things.” 2009. https://martinfowler.com/bliki/TwoHardThings.html.               Zimmermann, Olaf, Mirko Stocker, Daniel Lübke, Uwe Zdun, and Cesare Pautasso. 2022. Patterns for API Design: Simplifying Integration with Loosely Coupled Message Exchanges. Addison-Wesley Signature Series (Vernon). Addison-Wesley Professional.        ","categories": [],
        "tags": [],
        "url": "/refactorings/makerequestconditional",
        "teaser": null
      },{
        "title": "Merge Endpoints",
        "excerpt":"also known as: Collapse Endpoints, Consolidate Operation-to-Endpoint Grouping   Context and Motivation   An API consists of multiple endpoints that expose one or more operations. Two or more of these operations are strongly related to each other. For instance, one can only be executed when the other one has succeeded, or one prepares the data that the other one works with. Because of their mutual dependencies, they have the same reasons to change and usually do so at the same time. This creates an undesired, rather tight coupling between them.   As an API provider, I want to bring the operations from multiple endpoints together so that they can be evolved jointly.   Stakeholder Concerns      #understandability (a.k.a. #explainability)   Fewer endpoints might be easier to understand and maintain — if they are cohesive and do not contain too many operations. The refactorings Move Operation and Rename Operation explain the quality attributes maintainability and understandability further.   #cohesion   See Wikipedia entry and SEBoK v. 2.4 for explanation of this general term.   #coupling   See Wikipedia entry and pattern summary of Loose Coupling.   Initial Position Sketch   The following MDSL endpoint snippet illustrates the starting position for this refactoring:   endpoint type Endpoint1 exposes    operation operation1      expecting payload \"RequestMessage1\"      delivering payload \"ResponseMessage1\"  endpoint type Endpoint2     exposes   operation operation2      expecting payload \"RequestMessage2\"      delivering payload \"ResponseMessage2\"   As the sketch above shows, the refactoring targets two endpoints and their operations.   Design Smells      Responsibility spread   The context description of the refactoring indicates that the single responsibility principle is violated. For instance, one stakeholder group might have to work with a large number of multiple endpoints to satisfy its information needs.   Extreme decomposition   The desire to decompose an API and its implementation into independently deployable units went too far. There are numerous endpoints exposing narrowly-scoped operations that call each other, or have to be orchestrated on the client side.   API does not get to the POINT   The I in POINT stands for Isolation. API operations should be free of unexpected side effects; they should not interfere with calls to other operations in the same or other APIs. See the blog post “APIs should get to the POINT” for further explanations.   Cloud-native traits violated   Cloud-Native Applications (CNAs) should have an adequate size and be modular. If two APIs depend on each other, this might not be the case. See the blog post “What is a Cloud-Native Application Anyway? 10 SUPER-IDEAL Application Properties and 7 Cloud-Native Traits” for a summary of this and other CNA traits.   Same backend system and/or domain data processed by multiple endpoints   One or more endpoints are not self-contained and autonomous, but depend on each other indirectly because they access one or more shared implementation resources (such as another system or a data store). Such “implementation spaghetti” violates several of the defining principles of service-oriented architectures and microservices, for instance independent deployability.   Instructions      Move all operations from the source endpoint to the target endpoint. If an intermediate mapping/configuration from endpoints to classes/methods exists in the Web frameworks that are used to implement the API, adjust both the configuration and the code.1   Delete the now orphaned/empty source endpoint from the code base.   Depending on the chosen evolution strategy, implement a new stub to redirect clients to the new endpoint (in HTTP, this can be achieved with URL redirection and status code 301).   Update the API test cases. Run them and compare the test results with those of the previous version to ensure that clients are served in the same way as before (both from a functional and from a non-functional point of view).   Update both API Descriptions (source and target) as well as the related Service Level Agreements.   Also update all API directories (registries, repositories) and/or portals (gateways, cluster managers, service meshes) that refer to source and target endpoints directly/statically.   Communicate these changes to the affected clients early and consistently.   Target Solution Sketch (Evolution Outline)   endpoint type Endpoint1AndEndpoint2Merged exposes    operation operation1      expecting payload \"RequestMessage1\"      delivering payload \"ResponseMessage1\"         operation operation2      expecting payload \"RequestMessage2\"      delivering payload \"ResponseMessage2\"   The complete MDSL source and target sketch is here. Its OpenAPI version can be found here.   Example(s)   Reverting the target example to the initial/source in Extract Operation qualifies as an example of this refactoring. After Extract Operation has been applied, the publication management example may provide the following command and query endpoints:2   Aggregate PublicationCommandsEndpoint {     Service PublicationManagementCommandFacade {         // a state creation/state transition operation:         @PaperId add(@PublicationEntryDTO newEntry);                  // computation operations (stateless):         String convertToBibtex(@PublicationEntryDTO entry);      } }  Aggregate PublicationQueriesEndpoint {     Service PublicationManagementQueryFacade {         // retrieval operations:         @PublicationArchive dumpPublicationArchive();         Set&lt;@PublicationEntryDTO&gt;lookupPublicationsFromAuthor(String writer);         String renderAsBibtex(@PaperId paperId);     } }   These two endpoints can be merged into one, leading to the following API design:   Aggregate PublicationEndpoint {     Service PublicationManagementFacade {         // a state creation/state transition operation:         @PaperId add(@PublicationEntryDTO newEntry);                  // retrieval operations:         @PublicationArchive dumpPublicationArchive();         Set&lt;@PublicationEntryDTO&gt;lookupPublicationsFromAuthor(String writer);         String renderAsBibtex(@PaperId paperId);                  // computation operations (stateless):         String convertToBibtex(@PublicationEntryDTO entry);      } }   Hints and Pitfalls to Avoid   Before and when applying this refactoring, make sure to:      Watch out for naming conflicts; if a conflict does occur, apply the Rename Operation refactoring before the merge.   In HTTP resource APIs, make sure that the merge does not cause any conflicts in the operation-to-verb mappings. Each resource, uniquely identified with a URI, is able to support one GET, one POST and one PUT operation (and so forth) only. Update the URI/resource naming scheme if required, for instance via sub-resources.   Do not take this refactoring to the extreme; we do not advise creating one endpoint per stakeholder group.   The source and the target endpoints may have different security requirements. For instance, some operations may require rather fine-grained, attribute-based authorization while others use role-based authorization (or none). Another case might be one endpoint using basic authentication and API Keys and the other one using OpenID Connect and OAuth. Such situations can either be (viewed as) opportunities to improve the weaker security solution, or as threats because they could cause unexpected/undesired complexity. It is important to identify such mismatches before applying the refactoring and make a conscious architectural decision whether to merge or not.   Related Content   This refactoring undoes Extract Operation. A simpler form of it is Move Operation. Viewed at the code level, Inline Class is a related refactoring that can be used to merge the implementation classes of the endpoints.   There are different reasons why tightly-coupled endpoints exist. If their role is data retrieval, they might provide access to the same underlying data, but for different clients. In that case, the Add Wish List refactoring might be suited.   The Service Cutter tool and method suggest sixteen coupling criteria such as “Semantic Proximity”, “Structural Volatility”, and “Security Contextuality” [Gysel et al. 2016]. These criteria are worth considering when merging endpoints and merging operations.   References             Gysel, Michael, Lukas Kölbener, Wolfgang Giersche, and Olaf Zimmermann. 2016. “Service Cutter: A Systematic Approach to Service Decomposition.” In Service-Oriented and Cloud Computing - 5th IFIP WG 2.14 European Conference, ESOCC 2016, Vienna, Austria, September 5-7, 2016, Proceedings, edited by Marco Aiello, Einar Broch Johnsen, Schahram Dustdar, and Ilche Georgievski, 9846:185–200. Lecture Notes in Computer Science. Springer. https://link.springer.com/chapter/10.1007/978-3-319-44482-6_12.                        An example of such framework is Play. &#8617;                  The notation in this example is CML, the tactic domain-driven design language supported by Context Mapper. &#8617;           ","categories": [],
        "tags": [],
        "url": "/refactorings/mergeendpoints",
        "teaser": null
      },{
        "title": "Merge Operations",
        "excerpt":"also known as: Colocate Responsibilities, Consolidate Operations   Context and Motivation   An API endpoint contains two operations with similar, possibly overlapping responsibilities. Typically, this was not the intention of the original API designers, but the result of an API evolution process. For instance, the API provider might have added a new operation to the endpoint instead of adjusting an existing one.   As an API designer, I want to remove an API operation from an endpoint and let another operation in that endpoint take over its responsibilities so that there are fewer operations to maintain and evolve and the inner cohesion of the endpoint improves.   Stakeholder Concerns      #understandability, #explainability, #learnability   There are many reasons to change an API (not just refactorings) [Stocker and Zimmermann 2021]. APIs should be changed as much as required and as little as possible, and ripple effects be avoided. One of the first steps in related maintenance tasks is to determine which parts of the system should be changed. An API whose endpoints have clearly identified roles helps developers to quickly understand the API.   #cohesion   Cohesive design elements (here: API operations in an endpoint) belong together naturally because they share certain properties. In an API design context, the security rules that apply are an example of such a property. Cohesion is desirable because it makes the system easier to understand and maintain. ISO/IEC/IEEE 24765 [Standardization et al., n.d.] defines cohesion as “manner and degree to which the tasks performed by a single software module are related to one another.”   #coupling   In our context, coupling is a measure of how closely connected and dependent on each other endpoints and their operations are. The coupling may concern the data exchanged and/or the operation call sequencing. See Wikipedia entry for Coupling and Loose Coupling pattern for general explanations.   Initial Position Sketch   The refactoring is applicable if the current API exposes an endpoint with at least two operations, as shown by the following MDSL snippet:   endpoint type Endpoint1BeforeMerge exposes    operation operation1      expecting payload \"RequestMessage1\"      delivering payload \"ResponseMessage1\"         operation operation2      expecting payload \"RequestMessage2\"      delivering payload \"ResponseMessage2\"   As the snippet shows, this refactoring targets a single endpoint and two of its operations. Figure 1 visualizes this initial position. The API offers two operations, which clients may or may not call in any particular order.      Figure 1: Merge Operations: Initial Position Sketch. The API provider offers two distinct operations (Op1, Op2). The client can invoke one (Request 1, Response 2) or the other (Request 3, Response 4). Some metadata and other data elements are exchanged.   Design Smells      Responsibility spread   Endpoint roles and/or operation responsibilities are rather diffuse; the Single Responsibility Principle is violated. For instance, API clients serving a particular stakeholder have to call multiple operations to satisfy their information needs. Another example would be that a choreographed or orchestrated business process implementation has to consult too many distributed operations to fulfill its job.   High coupling   Two or more operations perform narrowly focused, rather low-level activities. Clients have to understand and combine all of these activities to achieve higher goals, leading to a degraded developer experience and coordination needs. This causes these operations to be coupled with each other implicitly.   REST principle(s) violated   The “Uniform interface” is an important design constraint imposed by the REST style that many HTTP APIs employ. REST mandates using the standard HTTP verbs (POST, GET, PUT, PATCH, DELETE, etc.), which are associated with additional constraints. For instance, GET and PUT requests must be idempotent to be cachable [Allamaraju 2010]. Sometimes, mismatches between the API semantics and the REST constraints can be observed; sometimes, the REST constraints limit extensibility (for instance, when a resource identified by a single URI runs out of verbs) [Serbout et al. 2022].   Instructions   This refactoring requires careful planning and execution:      Merge the data structures used in the two request messages if they differ. A straightforward approach is to combine and wrap the original message contents in a new DTO (see Introduce Data Transfer Object) in the consolidated message.   (Optional) Add a boolean flag to the request message to distinguish the merged operations, for instance, to dispatch the request to the proper API implementation logic. This step is optional because it might be possible to inspect the merged request message to select the implementation logic (see the example later).   Merge the response message data structures as well. A DTO can be used to do so.   Consolidate the implementation code, deciding how to route incoming requests and how to prepare the consolidated response.   Add at least two tests if the boolean flag introduced in Step 2 is present: one sets it to false, and the other sets it to true. Test the new API and compare old and new behavior.   Update supporting artifacts such as API Description and usage examples. Show how to use the boolean flag (if introduced) and explain how to migrate from the old to the new API.   Inform the API user community about the change and its rationale. Make the news item self-contained or provide direct links to the updated API Description and usage examples; avoid general statements such as “We have changed our API in an incompatible way. Please consult the documentation to learn how.”   The changes introduced in Steps 1 to 4 are not backward-compatible per se. Steps 5 to 7 apply to most refactorings in our catalog; we refer to them as TELL (Test, Explain, Let Know, and Learn).   Target Solution Sketch (Evolution Outline)   The API contract from the Initial Position Sketch above still contains one endpoint. But only one operation is present now (note that { , } is the MDSL notation for Parameter Trees [Zimmermann et al. 2017], an abstraction of JSON objects):   data type ConsolidatedRequestMessage {     \"RequestMessage1\", \"RequestMessage2\" } data type ConsolidatedResponseMessage {     \"ResponseMessage1\",  \"ResponseMessage2\" }  endpoint type Endpoint1AfterMerge exposes    operation operation1and2Merged      expecting payload          \"RequestMessage12\":ConsolidatedRequestMessage     delivering payload          \"ResponseMessage12\":ConsolidatedResponseMessage   Figure 2 visualizes the resulting API design that uses a Content-Based Router to select the operation to execute.      Figure 2: Merge Operations: Target Solution Sketch. The client sends a request (1) that includes one or more data elements. This figure shows two data elements, but they might not all be mandatory. The provider uses a Content-Based Router to execute operations (Op1, Op2) depending on the content of the message and returns a response, for example, some metadata (2).   Example(s)   In the following example of the user administration endpoint of an API implemented in Spring Boot, there are two operations to change the e-mail address and username, respectively. Both use the same endpoint /users/{id}, but the developer decided to use different HTTP verbs (POST and PATCH) to implement the two operations:   @PostMapping(\"/users/{id}\") public ResponseEntity&lt;User&gt; changeEmail(     @RequestBody ChangeEmailDTO changeEmailDTO)  {     log.debug(\"REST request to change email : {}\",          changeEmailDTO);     ... }  @PatchMapping(\"/users/{id}\") public ResponseEntity&lt;User&gt; changeUsername(     @RequestBody ChangeUsernameDTO changeUsernameDTO) {     log.debug(\"REST request to change username : {}\",          changeUsernameDTO);     ... }   Keep in mind that the API client does not see these method names but the POST and PATCH verbs only. Using different HTTP verbs simply to distinguish between two operations violates REST principles. POST is meant for creating resources, not updating them, as done in changeEmail. These endpoints can then be used as follows:   curl -X POST  api/users/123 -d '{ … }' curl -X PATCH api/users/123 -d '{ … }'   The HTTP verb used is the only difference from the perspective of the API client; the fixed amount of available HTTP verbs limits future extensibility given (maybe passwords should also be changeable?). Hence, it is decided to merge the two operations and create a composite request message DTO:   class ChangeUserDetailsDTO {     ChangeEmailDTO changeEmail;     ChangeUsernameDTO changeUsername; }  @PatchMapping(\"/users/{id}\") public ResponseEntity&lt;User&gt; changeUserDetails(     @RequestBody ChangeUserDetailsDTO changeUserDetailsDTO) {     if (changeUserDetailsDTO.changeEmail != null) {         log.debug(\"REST request to change email : {}\",              changeUserDetailsDTO.changeEmail);         ...     }     if (changeUserDetailsDTO.changeUsername != null) {         log.debug(\"REST request to change username : {}\",              changeUserDetailsDTO.changeUsername);         ...     }     ... }   Further operations changing other properties of the user can now be implemented by extending the DTO without introducing new operations or even endpoints. The DTO content determines the nature of the change; no boolean parameter was needed in this example. Clients can now also initiate several changes in a single request.   Hints and Pitfalls to Avoid   Merging operations is more challenging than merging endpoints (which usually merely group operations under a unique address such as a parent URI):      The operations to be merged must appear in the same endpoint. Apply Move Operation first if needed.   Do not break HTTP verb semantics when merging (in HTTP resource APIs). For instance, idempotence might get lost if a replacing PUT and an updating PATCH are merged.   When merging the request and response messages, decide where and how the merged messages embed the original message elements. Some data exchange formats have first-class concepts for choices. If this is not the case, the optionality of list items combined with feature flags/toggles can be used.   The complexity of the implementation logic and the tests increases when merging operations: The implementation logic must distinguish between the merged operations. The tests must cover all possible combinations of the merged operations.   Implementing this refactoring in a backward-compatible way is not trivial because of the changes imposed on request and response messages. One tactic could be to provide a new operation for the merged functionality and keep the original ones in place. The original ones can then forward incoming requests to the new operation, wrapping and un-wrapping request and response messages.   See the Merge Endpoints refactoring for Confidentiality, Integrity, and Availability (CIA) considerations; inconsistent or inappropriate CIA settings (authentication, authorization) are less likely to result from this refactoring (depending on how the API endpoint and operations have been identified) but are still worth considering.   Related Content   This refactoring reverts Split Operation. If the two operations do not reside in the same endpoint, an upfront Move Operation refactoring can prepare its application.   The Service Cutter tool and method suggest sixteen coupling criteria [Gysel et al. 2016]. These criteria primarily apply when merging endpoints but are also worth considering when merging operations.   References             Allamaraju, Subbu. 2010. RESTful Web Services Cookbook. O’Reilly.               Gysel, Michael, Lukas Kölbener, Wolfgang Giersche, and Olaf Zimmermann. 2016. “Service Cutter: A Systematic Approach to Service Decomposition.” In Service-Oriented and Cloud Computing - 5th IFIP WG 2.14 European Conference, ESOCC 2016, Vienna, Austria, September 5-7, 2016, Proceedings, edited by Marco Aiello, Einar Broch Johnsen, Schahram Dustdar, and Ilche Georgievski, 9846:185–200. Lecture Notes in Computer Science. Springer. https://link.springer.com/chapter/10.1007/978-3-319-44482-6_12.               Serbout, Souhaila, Cesare Pautasso, Uwe Zdun, and Olaf Zimmermann. 2022. “From OpenAPI Fragments to API Pattern Primitives and Design Smells.” In 26th European Conference on Pattern Languages of Programs. EuroPLoP’21. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3489449.3489998.               Standardization, International Organization for, International Electrotechnical Commission, Institute of Electrical, and Electronics Engineers. n.d. ISO/IEC/IEEE 24765: 2017(e): ISO/IEC/IEEE International Standard - Systems and Software Engineering–Vocabulary. IEEE Std. IEEE.               Stocker, Mirko, and Olaf Zimmermann. 2021. “From Code Refactoring to API Refactoring: Agile Service Design and Evolution.” In Service-Oriented Computing, edited by Johanna Barzen, 174–93. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-87568-8_11.               Zimmermann, Olaf, Mirko Stocker, Daniel Lübke, and Uwe Zdun. 2017. “Interface Representation Patterns - Crafting and Consuming Message-Based Remote APIs.” In 22nd European Conference on Pattern Languages of Programs (EuroPLoP 2017), 1–36. https://doi.org/10.1145/3147704.3147734.        ","categories": [],
        "tags": [],
        "url": "/refactorings/mergeoperations",
        "teaser": null
      },{
        "title": "Move Operation",
        "excerpt":"also known as: Relocate API Call, Balance Endpoint Responsibilities, Adjust Service Decomposition   Context and Motivation   One or more API endpoints (for instance, HTTP resources) have been developed, tested, and deployed. One endpoint contains multiple operations (for instance, HTTP PUT and POST methods). Not all of these operations work with the same domain concept(s) in the underlying API implementation; their responsibilities differ with regards to stakeholder groups and their concerns. Some operations are process-oriented activities while others have a data-oriented information management character; their quality characteristics including data protection needs may differ. As a consequence, the endpoint serves multiple roles in the API architecture and the operations have diverging responsibilities. There is a risk that they drift further apart during API evolution.   As the API provider, I want to focus and consolidate the operations of an endpoint on a single role and purpose so that API clients serving a particular stakeholder group understand the API design intuitively and the provider has a single reason to change each endpoint.   Stakeholder Concerns      #maintainability and #understandability (a.k.a. explainability)   There are many reasons to change an API (not just refactorings) [Stocker and Zimmermann 2021]. APIs should be changed as much as required and as little as possible, and ripple effects be avoided. One of the first steps in related maintenance tasks is to determine which parts of the system should be changed. An API whose endpoints have clearly separated concerns and distinguished roles helps developers to quickly understand the API.   #scalability and #reliability   When co-located in a single endpoint and deployed jointly, operations might influence each other. For instance, if one of these operations is long-running or causes a provider-internal error, its siblings might suffer from quality-of-service degradation too.   #single-responsibility-principle and #independent-deployability   Architectural principles such as single responsibility and independent deployability can guide the selection of refactoring. Some are affected positively, others negatively. The POINT principles for API design (purposeful, style-oriented, isolated, channel-neutral, and T-shaped) belong to this category; moving an operation to another endpoint can improve P, O, and I (but might harm T when looking at a single endpoint and not an entire API). Some principles might be style-defining (for instance, statelessness of interactions in REST), which makes it even more important to adhere and stick to them during design and evolution.   Initial Position Sketch   The following platform- and technology-independent API design, specified in MDSL, serves as the starting point for this refactoring:   endpoint type SourceEndpoint exposes    operation operation1      expecting payload \"RequestMessage1\"      delivering payload \"ResponseMessage1\"   operation operation2      expecting payload \"RequestMessage2\"      delivering payload \"ResponseMessage2\"  endpoint type TargetEndpoint     // zero or more already existing operations   The refactoring targets are:      A single API endpoint   At least one of its operations (there should be more than one)   An already existing second endpoint (creating a new one is described in Extract Operation)   Design Smells      God endpoint   One endpoint contains a large number of operations that do not serve related purposes.   Role and/or responsibility diffusion   The endpoint is both an Information Holder Resource and a Processing Resource. It is hard to explain what it does coherently. For instance, some of the exposed data lives long and changes rarely; other data goes through many create, update, delete operations in a short time span.   Data lifetime mismatches   Two operations in an endpoint deal with data that changes differently, both on the data definition and on the data manipulation level. For instance, one operation may expose master data and the other one may expose operational data. This causes undesired constraints on endpoint evolution.   Too coarse-grained security or data privacy rules   Some operations in an endpoint have more advanced quality requirements than others. For instance, some work with sensitive personal information that has to be protected, while others merely operate on public data. Other quality requirements mismatches might exist as well, for instance regarding availability and scalability.   Endpoint implementation spaghetti   There are several n to m relations between endpoints and implementation parts. At least one endpoint works with many implementation parts that evolve independently.   Instructions      Remove the operation from the API Description or mark it as deprecated and soon to be retired.   Refactor at the code level (for instance, Spring REST controllers when implementing the API in Java). Optionally, implement a new stub that merely redirects clients to the new endpoint operation (in HTTP, this can be achieved with URL redirection and status code 301).   Review the required and provided security policies including the access rights management (both in source and in target endpoint) and adjust as needed.   Test whether the source and the target endpoint still meet their contracts (both functional and non-functional characteristics).   Evaluate whether smells are removed (or at least some desired qualities have improved); record any remaining or new technical debt and identify additional follow-on refactorings.   Add the operation to the target API Description.   Inform all API clients about the completed change (in which version will it be introduced?) and provide migration information (or support the transition on a technical level, for instance with an HTTP redirect). Update all documentation accordingly.   Target Solution Sketch (Evolution Outline)   The following simple and abstract MDSL sketch shows the API design after the Move Operation steps have been performed:   endpoint type SourceEndpoint exposes    operation operation1      expecting payload \"RequestMessage1\"      delivering payload \"ResponseMessage1\"  endpoint type TargetEndpoint     exposes   // already existing operations (if any) still there   operation operation2     expecting payload \"RequestMessage2\"      delivering payload \"ResponseMessage2\"   The sketch does not show any semantics or qualities that smell (see the following example for that).   Example(s)   In a publication management system (such as JabRef), the remote service layer exposing a Solution-Internal API for Frontend Integration within a Web application might look like this:      Example of Refactoring “Move Operation” (as is)   The single service endpoint PublicationManagementFacade in the above design might be refactored into the following API design:      Example of Refactoring “Move Operation” (to be)   There are two endpoints now, PublicationManagementCommands and PublicationManagementQueries. The add and convertToBibtex operations appear in the new command endpoint. The other three methods remain in the already existing endpoint, which was renamed to PublicationManagementQueries.   Hints and Pitfalls to Avoid   It is worth checking (before and after the refactoring) the following:      Is the operation name still adequate in the new endpoint? If not, apply Rename Operation too.   Is the technology mapping of the request and response messages still ok? For instance, the MIME types in RESTful HTTP as well as the platform-specific authentication and authorization settings might have to be changed because the target endpoint receives an additional (sub-)responsibility.   Are new test cases and test data required in the target endpoint?   The effect of the new API design on the compatibility with the original one should be communicated. If needed, migration instructions and simple migration tools (such as scripts) may be provided.   Related Content   The refactoring Move Operation reverts itself. In code refactoring, Move Method [Fowler 2018] accomplishes a similar goal.   Consider Split Operation if the operation has several responsibilities and only some of them should be moved.   Rename Operation is related; so is Extract Operation.   The Design Practice Reference/Repository (DPR) Zimmermann and Stocker [2021] has a stepwise service design activity, which works with Candidate Endpoint Lists and Refined Endpoint Lists.   The API domain model that we refer to is introduced in Lübke et al. [2019].   References             Fowler, Martin. 2018. Refactoring: Improving the Design of Existing Code. 2nd ed. Addison-Wesley Signature Series (Fowler). Boston, MA: Addison-Wesley.               Lübke, Daniel, Olaf Zimmermann, Cesare Pautasso, Uwe Zdun, and Mirko Stocker. 2019. “Interface Evolution Patterns: Balancing Compatibility and Extensibility Across Service Life Cycles.” In Proceedings of the 24th European Conference on Pattern Languages of Programs. EuroPLop ’19. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3361149.3361164.               Stocker, Mirko, and Olaf Zimmermann. 2021. “From Code Refactoring to API Refactoring: Agile Service Design and Evolution.” In Service-Oriented Computing, edited by Johanna Barzen, 174–93. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-87568-8_11.               Zimmermann, Olaf, and Mirko Stocker. 2021. Design Practice Reference - Guides and Templates to Craft Quality Software in Style. LeanPub. https://leanpub.com/dpr.        ","categories": [],
        "tags": [],
        "url": "/refactorings/moveoperation",
        "teaser": null
      },{
        "title": "Relax Evolution Strategy",
        "excerpt":"also known as: Decrease Lifetime Guarantee, Soften Lifetime Commitment   Context and Motivation   An API runs in production. Its compatibility and extensibility characteristics are expressed in the form of one or more Evolution Patterns, including Experimental Preview, Aggressive Obsolescence, Limited Lifetime Guarantee, and Two in Production. These patterns differ in the way/frequency the provider is able to introduce breaking changes.1   As an API provider, I want to take back some of the rather strict lifetime, stability, and support guarantees given for an API endpoint and its elements so that I can update the API more freely and more often (including breaking changes).   Stakeholder Concerns      #maintainability   Reducing the number of versions that run in production and reducing their lifetime guarantees reduces the maintenance effort. See Tighten Evolution Strategy for more explanations.   #cost   Less code to be maintained over time means less maintenance costs. Sometimes, a planned (or managed) obsolescence approach is followed. Tighten Evolution Strategy also discusses this concern.   #flexibility   Not having to maintain different versions in parallel makes the provider team more flexible, but limits the options client developers have. Again, Tighten Evolution Strategy features this quality as well.   “Interface Evolution Patterns — Balancing Compatibility and Extensibility across Service Life Cycles” [Lübke et al. 2019] elaborates on the desired qualities that evolution patterns are generally confronted with (PDF).   Initial Position Sketch   The following provider specification does not yet define any life cycle guarantee (notation: MDSL):   API provider SampleAPIProvider1    offers SomeDemoContract      at endpoint location \"http://www.testdomain.io:80/path/subpath\"     via protocol HTTP binding resource SampleAPIProvider1Resource      Relax Evolution Strategy: Initial Position Sketch   Targets:      API endpoint and its API Description (business and technical contract).   Service Level Agreement (SLA) that accompanies or is part of API Description.   Design Smells      Feature/release inertia a.k.a. stale roadmap   Providers are reluctant to introduce new features because of the commitments made and guarantees given to existing API users (for example, in an SLA).   Resistance to change caused by uncertainty   A provider has made many assurances/guarantees to clients and is now reluctant to apply other API refactorings because it is afraid of disrupting these clients.   Instructions   Relaxing guarantees might be more subtle than tightening them. Therefore, fewer transitions make sense:      Reduce the number of supported versions from N In Production to N-1 in Production.   No longer support Two in Production but still give a Limited Lifetime Guarantee for a single API version.   Replace a Limited Lifetime Guarantee with an Aggressive Obsolescence policy.   Revert from an (unrealistic, theoretical) Eternal Lifetime Guarantee to a (fixed, predefined) Limited Lifetime Guarantee.      Note that we do not recommend to go down to an Experimental Preview.   Target Solution Sketch (Evolution Outline)   The life cycle guarantee primarily is a documentation item (first and foremost):      Relax Evolution Strategy: Target Solution Sketch   Example(s)   When relaxing the lifetime guarantees, an MDSL contract may change from:   API provider SampleAPIProvider1 version 1.0.0   offers SomeDemoContract      at endpoint location \"http://www.testdomain.io:80/path/subpath\"     via protocol HTTP    binding      resource SampleAPIProvider1Resource   provider governance LIMITED_LIFETIME_GUARANTEE   to:   API provider SampleAPIProvider1 version 1.1.0   offers SomeDemoContract      at endpoint location \"http://www.testdomain.io:80/path/subpath\"     via protocol HTTP    binding      resource SampleAPIProvider1Resource   provider governance AGGRESSIVE_OBSOLESCENCE   The Evolution Patterns on the MAP website list real-world known uses of the patterns (and related changes over time).   Hints and Pitfalls to Avoid   Apply this refactoring when existing clients complain, for instance about the slow speed of API innovations.   When applying the refactoring, make sure to:      Perform a cost-benefit analysis, preferably a quantitative rather than a qualitative one.   Minimize incompatible changes and apply Semantic Versioning when releasing.   Communicate changes early, clearly, and continuously so that clients are not surprised (TELL).   Introduce Version Mediator can be applied if lifetime guarantees are weakened and some clients are unable to migrate to a newer, incompatible version. Introduce Version Identifier might have to be applied first so that clients can learn about versions and their compatibility characteristics.   Related Content   This refactoring is the opposite of Tighten Evolution Strategy. Both these actions actually do not change the API structure, but its developmental qualities and support properties.   Microservice Domain-Specific Language (MDSL) specifications have an optional SLA section and a provider governance section in their API provider parts, which is explained in the language documentation.   References             Lübke, Daniel, Olaf Zimmermann, Cesare Pautasso, Uwe Zdun, and Mirko Stocker. 2019. “Interface Evolution Patterns: Balancing Compatibility and Extensibility Across Service Life Cycles.” In Proceedings of the 24th European Conference on Pattern Languages of Programs. EuroPLop ’19. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3361149.3361164.                        Alternatively (but only justified in rare cases), the provider might decide not to do so at all and guarantee an Eternal Lifetime Guarantee. &#8617;           ","categories": [],
        "tags": [],
        "url": "/refactorings/relaxevolutionstrategy",
        "teaser": null
      },{
        "title": "Rename Endpoint",
        "excerpt":"Context and Motivation   An API endpoint runs in production and is used by clients. Its build time name and runtime address do not match its domain semantics well.   As an API developer, I want to express the domain concepts exposed by an endpoint and the architectural role that it plays as accurately as possible in the published language and the API contract.   Stakeholder Concerns      #understandability (a.k.a. #explainability, #learnability)   A good name expresses what an endpoint has to offer, which helps clients decide whether and how to use it. Rename Operation has more explanations of this quality.   #learnability (readability in particular)   The more expressive and meaningful a name is, the easier it is to navigate through the code base, for instance when searching for them while fixing bugs and adding features. Move Operation has more explanations of this quality.   Initial Position Sketch   Let’s assume that an endpoint type called OldCrypticName exists:   endpoint type OldCrypticName exposes    operation operation1      expecting payload \"SomeRequestMessage\"   As the sketch shows, the refactoring targets a single endpoint that contains one or more operations.   Design Smells      REST principle(s) violated   Abstract API endpoints correspond to resources identified by URIs in RESTful HTTP APIs. Their names should convey the meaning and role of the resources. For instance, using verbs as relative paths might be considered an antipattern; resources should be named with nouns that can be traced back to domain entities (note that these entities and their resource representations can be represented as Processing Resources and/or data-oriented Information Holder Resources, both in REST and in APIs leveraging other integration styles).   Cryptic or misleading name   The chosen name is not straightforward to understand or is not part of the domain terminology. It raises wrong, unexpected or undesired associations.   Instructions   This refactoring is rather straightforward to apply.1 The main task is to find a suited name that balances the desired qualities.      Discuss alternative new names and decide for one. Have this decision reviewed and agreed upon.   Apply code-level rename refactorings, for instance for the controller class realizing the endpoint.   For backward compatibility, a redirect can be implemented. Either at the protocol level (for instance with an HTTP redirect) or at the code level by keeping the old method and calling the new one from it.   Update security rules, tests and documentation as well as any address resolution services (gateways, portals, registries, repositories, service meshes).   Target Solution Sketch (Evolution Outline)   The endpoint type is called NewExpressiveName now:   endpoint type NewExpressiveName exposes    operation operation1      expecting payload \"SomeRequestMessage\"    Example(s)   In the publication management example, an application of the refactoring might go from:   Aggregate APIEndpoint {     Service JabRefDatabaseFacade {         Set&lt;@PublicationEntryDTO&gt;lookupPublicationsFromAuthor(String writer);         ...     } }   to:   Aggregate PublicationManagementEndpoint {     Service PublicationManagementFacade {         Set&lt;@PublicationEntryDTO&gt;lookupPublicationsFromAuthor(String writer);         ...     } }   The specific name of an open source tool, JabRef, was replaced by a more general, domain-specific name. This reduces the prerequisite knowledge that stakeholders of the name (such as API client developers, but also API implementation maintainers and API product managers and testers) have to have.   Hints and Pitfalls to Avoid   Here is some advice regarding naming:      Organization-, unit-, or project-wide naming conventions should be considered.   Strategic Domain-Driven Design might be able to contribute naming suggestions as well.   Naming collisions with other endpoints should be avoided (even if they are technically possible, which depends on the chosen protocol technology).   URI templates {id} in HTTP resource APIs also require consistent naming (and these names may appear in multiple places in API specifications and implementations).   More hints can be found in the Rename Operation and Rename Representation Element refactorings.   Related Content   Rename Operation and Rename Representation Element also address readability, but have other targets.   Refactoring.Guru calls for Clean code and emphasizes that poor names make code difficult to grasp.   “The Art of Readable Code” [Dustin Boswell 2011] contains many helpful hints on naming program elements that also apply to APIs.   References             Dustin Boswell, Trevor Foucher. 2011. The Art of Readable Code. O’Reilly Media, Inc.                        assuming that the used tools (including IDEs, API contract editors, and test tools) support consistent find-and-replace across files, and do so conveniently and reliably &#8617;           ","categories": [],
        "tags": [],
        "url": "/refactorings/renameendpoint",
        "teaser": null
      },{
        "title": "Rename Operation",
        "excerpt":"Context and Motivation   An API endpoint, for instance, an HTTP resource, has been developed, tested, and deployed. The name of one of the operations of the endpoint does not represent its semantics well; there is a mismatch between the operation name and the performed operation. It is not easy to comprehend.   As an API provider, I want to express the responsibilities of an operation in its name so that client developers, API developers, operators, and non-technical stakeholders such as end users and product managers understand the API — and each other in conversations about the API.   Stakeholder Concerns      #maintainability   APIs should be changed as much as required and as little as possible, and ripple effects be avoided; source code and documentation on client and provider side using a particular name have to be updated if this name changes. Expressive operation names help with orientation during API evolution. Debugging and trouble shooting is also easier if logs and error reports contain meaninfgul names.   #understandability (incl. #explainability, #learnability)   All stakeholders involved in development and operations should be able to grasp what an API is supposed to do (and actually does) with ease; it should be straightforward to teach API usage. On the contrary, educated guesses and implicit assumptions are likely to cause misunderstandings that lead to technical risk and defects later on.   Initial Position Sketch   This refactoring deals with a single operation.   This is a rather trivial initial design to be improved with this refactoring, specified in the Microservice Domain-Specific Language (MDSL) notation:   endpoint type GenericEndpointOfUnknownRole exposes    operation hardToGraspName      expecting payload \"SomeRequestMessage\"    It is unlikely that an identifier such as hardToGraspName is part of the vocabulary of any application domain or genre that the API deals with (such as finance, e-commerce/retail, or distributed control system in a factory).   Design Smells      Curse of knowledge   The operation name is easy to comprehend – but only for the developers of the API implementation on the provider side. On the contrary, client developers miss required context information.1   Role and/or responsibility diffusion   The operation is doing something, but the effects of the operation execution are not clear. For instance, it is not specified whether it reads and/or writes provider-side data and application state. The domain model abstractions/concepts that it works with remain fuzzy. Precision might be harmed and ambiguities introduced.   Ill-motivated naming conventions   Knowing what no one else knows could be seen as a pragmatic approach to job security; obscuring operation names might be part of such a strategy. However, the attitude driving such naming decisions can be considered unprofessional or unethical [Zimmermann, Stocker, and Kapferer 2024]; API design and documentation should be seen as a service provided to the client community (and other stakeholders).   Sloppy naming   Another example of good intentions gone wrong is trying to be funny when naming program(ming) artifacts; endpoint and operation names are not the most suited places for humor or irony because they distract from the facts.   Cryptic or misleading name   The name of the operation is not only difficult to understand but also misleading. It might suggest that the operation does something that it does not, which may have been caused by a change in the implementation of the operation without updating the name.   Instructions      Rename the operation in the abstract API contract and any technical API Description (for instance, its OpenAPI description).   Refactor on the code level; for instance, apply “Rename Element” or “Rename Method” as offered by many Java IDEs. Optionally, implement a new stub that merely redirects clients to the new endpoint operation; in HTTP, this can be achieved with URL redirection and status code 301 [Fielding and Reschke 2014].   Adjust the test cases and run them (to keep the builds “green”).   Update all supporting documentation such as API reference and guides, examples, and tutorials. Check and update security rules if necessary.   Inform all known API clients about the change, ideally with detailed instructions how to migrate. Code snippets that can be copy-pasted easily will be appreciated by the client maintainers.   Target Solution Sketch (Evolution Outline)   The following MDSL sketch outlines how to improve the naming on an abstract, conceptual level:   endpoint type DomainNounAndRDDRoleStereotype exposes    operation verbFromDomainLanguage      expecting payload \"DomainLevelTransferObject1\"      delivering payload \"DomainLevelTransferObject2\"   Note that two similar refactorings were applied as well, Rename Endpoint and Rename Representation Element.   Example   In a publication management system, the remote service layer of a Web application might expose a Community API for Backend Integration, two foundational API patterns. The service might look as follows:   Service JabrefAPI {     @PaperId add(@PublicationEntryDTO newEntry);   The notation in this example is CML, the tactic domain-driven design language supported by Context Mapper. Renaming the rather generic names yields this API design:   Service PublicationManagementFacade {   // a state creation/state transition operation   // (DTO = Data Transfer Object)   @PaperId addPublicationToArchive(     @PublicationEntryDTO publicationInformation);   Endpoint name, operation name, and parameter name are now free of technical jargon (except for the pattern names Facade and DTO, with the acronym being explained in the comment).   Hints and Pitfalls to Avoid   Consider consulting the following artifacts (before and after the refactoring):      Naming conventions on organization, unit, or project level. If you cannot locate such conventions, use this opportunity to establish them. Rename consistently and document the rationale for your naming decisions. For instance, such conventions might state that operation names start with verbs and followed by a noun from the domain vocabulary (see example above).   Glossaries and the Ubiquitous Language established by a domain model [Zimmermann and Stocker 2021]. Note that some community members advise that BankAccountAggregate is a bad choice of name while others recommend this domain-pattern pairing convention.   Coding guidelines, both general and language-specific.   Avoid special characters such as underscore _ in operation names because middleware and tools might not handle them correctly. The same holds for natural language-specific characters such as German “Umlaute”.2   Be careful with metaphors when naming things. Some might not be understood by parts of the target audience, others might cause unwanted reactions. Baseball fans, for example, know what a “curveball” and a “pitcher” are, but this sport is not as global as others. Apply the ones that you do choose consistently and do not mix them wildly.3 Ask yourself: Would the current name also work in another domain? In how many projects/APIs can this name be found?   Note that this refactoring is not straightforward to apply in HTTP resource APIs due to the REST principle “uniform interface”: One cannot rename the predefined HTTP PUT verb to something that has domain semantics. This constraint represents a deliberate design decision and is inherent to/in the REST style. It is one reason the Web works: it is not necessary to recompile the Web browser when there is a breaking change in the HTML layout of a website. URIs, however, can be changed; hence, the Rename Endpoint refactoring often is eligible.   Related Content   This refactoring reverts itself. In code refactoring, there is Rename Method [Fowler 2018].   Move Operation is another operation-level refactoring.   The hints in “The Art of Readable Code” [Dustin Boswell 2011] also apply to API naming. Many programming language communities also have naming guidelines, such as the C++ Core Guidelines feature naming suggestions. A CppCon 2019 talk by Kate Gregory titled “Naming is Hard: Let’s Do Better” has good advice that applies when choosing API element names.   References             Dustin Boswell, Trevor Foucher. 2011. The Art of Readable Code. O’Reilly Media, Inc.               Fielding, Roy T., and Julian Reschke. 2014. “Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content.” Request for Comments. RFC 7231; RFC Editor. https://doi.org/10.17487/RFC7231.               Fowler, Martin. 2018. Refactoring: Improving the Design of Existing Code. 2nd ed. Addison-Wesley Signature Series (Fowler). Boston, MA: Addison-Wesley.               Gamma, Erich, Richard Helm, Ralph Johnson, and John Vlissides. 1995. Design Patterns: Elements of Reusable Object-Oriented Software. Addison-Wesley.               Zimmermann, Olaf, and Mirko Stocker. 2021. Design Practice Reference - Guides and Templates to Craft Quality Software in Style. LeanPub. https://leanpub.com/dpr.               Zimmermann, Olaf, Mirko Stocker, and Stefan Kapferer. 2024. “Bringing Ethical Values into Agile Software Engineering.” In Smart Ethics in the Digital World: Proceedings of the ETHICOMP 2024. 21th International Conference on the Ethical and Social Impacts of ICT, 90–93. Universidad de La Rioja.                        The term “curse of knowledge” originates from technical writing. See, for instance, hint 5 in the blog post Technical Writing Tips and Tricks and a video lecture by Steven Pinker referenced in that post. &#8617;                  As a test, do ä, ö, ü render properly when you read this? &#8617;                  What happens if an elephant enters a room as a Visitor or crosses a Bridge? Should the Flyweight pattern be applied then? Or does it make sense to build a Factory in this case? What will Observers think about this Strategy? [Gamma et al. 1995] &#8617;           ","categories": [],
        "tags": [],
        "url": "/refactorings/renameoperation",
        "teaser": null
      },{
        "title": "Rename Representation Element",
        "excerpt":"also known as: Rename Parameter, Rename Payload Part   Context and Motivation   An API endpoint operation expects data from clients and/or delivers data to them. The data representations are structured, and certain parts of the structural elements are named (for instance, keys in key-value pairs or type definitions).   As a data modeler and/or Data Transfer Object (DTO) designer, I want to use expressive and domain-specific terminology so that data representations and their elements become self-explanatory to API developers on the client as well as on the provider side.   Stakeholder Concerns      #understandability, #explainability, #learnability   A well-chosen name expresses what a representation element (such as a JSON object with its properties) has to offer, which helps clients decide whether and how to use it. The named element has a single meaning and purpose.   #maintainability   The more expressive and meaningful a name (for instance, the key in a key-value pair) is, the easier it is to search and navigate the code base, for instance, when fixing bugs and adding features during API evolution. Split Operation covers this quality in more detail.   Initial Position Sketch   Let us assume the following simple endpoint design (notation: abstract MDSL):   endpoint type Endpoint1 exposes    operation operation1      expecting payload {       \"v1\":D&lt;string&gt;,       \"v2\":D&lt;int&gt;}     // one-way exchange in this example,      // so no response message   As the sketch shows, the refactoring targets a single endpoint and one of its operations. Presumably, the v in v1 and v2 stands for “value”, but this does not get clear from the terse one-letter, one-number acronyms v1 and v2.   Design Smells      Cryptic or misleading name   The chosen element name is difficult to understand for stakeholders unfamiliar with the API implementation. For instance, it might not be part of the agreed-upon domain vocabulary or unveil implementation details such as column names in database tables. It might also be ambiguous and overloaded with different meanings (in the same context).   Security by obscurity   Sometimes, it is argued that unlabeled, undocumented data is harder to tamper with. But such a tactic alone does not qualify as a sound security solution. It harms maintainability because it increases the risk of introducing bugs because of a lack of clarity for maintainers and auditors, not just attackers.   Change log jitter or commit chaos   The name has been, and continues to be, frequently changed according to the logs kept by the version control system. Frequent changes indicate that the domain language is not yet stable or has not yet been defined, communicated, and agreed upon sufficiently.   Leaky encapsulation and high coupling   Program-internal names or identifiers might accidentally have leaked into the API. For example, the initial API could have been generated from internal classes. For API clients, such internal names might be hard to understand.   Instructions   This refactoring is rather straightforward to apply:      Discuss alternative new names and decide on one. Have this decision reviewed and agreed upon.   Apply code-level rename refactorings; do not forget to update code-level comments when doing so. If the code is generated from a specification, update the API design specification and then re-generate the code.   Update API tests and API Description.1   Several strategies for backward compatibility exist. The implementation could continue to accept the old names as well. However, this approach can quickly become unwieldy if many such changes accumulate over time. By applying the Introduce Version Mediator refactoring, clients can continue using the old names which will then get translated by the version mediator to the new names.   Target Solution Sketch (Evolution Outline)   The element names v1 and v2 in the initial position have been replaced by customerName and customerID in this MDSL sketch:   endpoint type Endpoint1 exposes    operation operation1      expecting payload {       \"customerName\":Data&lt;string&gt;,       \"customerID\":Data&lt;int&gt;     }    Arguably, customerName is more expressive than v1 unless the representation element is a simple loop counter (which does not seem to be the case). This renaming makes the API specification easier to understand, assuming that a common understanding of the term “customer” has been reached within a context and community. operation1 remains generic in the above sketch, which makes it a candidate target for Rename Operation.   Example(s)   In the Policy Management backend of the Lakeside Mutual sample application, we find a Data Transfer Object (DTO) called InsuranceQuoteDto, which is exposed in the Web API that is provided by a RESTful HTTP controller:   public class InsuranceQuoteDto {     @Valid     @NotNull     private Date expirationDate;      @Valid     @NotNull     private MoneyAmountDto insurancePremium;      @Valid     @NotNull     private MoneyAmountDto policyLimit;      ... }   The name InsuranceQuoteDto as well as expirationDate, insurancePremium, and policyLimit are all expressive names; names such as inputData, date, and amount would be less domain-specific and could be renamed with this refactoring.   One could argue that abbreviations and technical terms should be avoided in API naming. Removing Dto from the name in a refactoring would make the name shorter and cleaner but also hide the architectural role played by the class and go against the “architecturally-evident coding style” recommended by Fairbanks [2010]. In this particular case, the acronym is explained in a comment in the source code.   Hints and Pitfalls to Avoid   Coming up with good names is challenging, and some authors consider it one of the hardest problems in software engineering [Benner 2023]. When choosing names, keep in mind that good names should:      Precisely reveal the purpose/intent of the named element.   Be intelligible, so they do not need to be deciphered first.   Be pronounceable so that they can be talked about.   Be as simple and short as possible, but not shorter.   Adhere to the conventions of the interface specification and/or programming language in use.   When applying this refactoring, one should promote a controlled amount of domain vocabulary into the published language of the API. Optionality and value ranges should be explained in the API documentation. Comments in machine-readable specifications and specification documents targeting humans are valid locations for this information. The reuse of already existing data structures and standard vocabularies may be considered if that is permitted, for instance, microformats or schema.org2.   Mapping implementation-level data structures one-to-one introduces undesired tight coupling between API and implementation and should therefore be avoided. Implementation-level data structures should be wrapped or mapped (see Introduce Data Transfer Object) to hide implementation details.   It makes sense to avoid technical jargon (in particular jargon that might go out of fashion soon or has a different meaning in other communities already). As already touched upon in the example, abbreviations should be avoided in names unless they are widespread in the API stakeholder community. Humor is a good thing, of course, but technical specifications are not necessarily a good place to (try to) be funny; ethical codes of conduct must not be violated when choosing names (e.g., [CoC:ACM]).   Related Content   Representation elements can be grouped by applying Introduce Data Transfer Object. Rename Operation and Rename Endpoint are available to rename API parts with a larger scope. The Id Element pattern [PatternsForAPIDesign:2022] makes naming suggestions (for instance, regarding uniqueness) and provides pointers to additional information. Data Element is the general pattern for any kind of representation element.   The book “The Programmer’s Brain: What every programmer needs to know about cognition” by Hermans [2021] has a chapter on “How to get better at naming things” that explains the cognitive processes involved in reading names in code and provides practical advice.   Fowler [2018] covers various rename refactorings, such as “Rename Variable” and “Rename Field.” And “The Art of Readable Code” [Dustin Boswell 2011] has helpful hints on naming program elements that also apply to APIs. Benner [2023] provides rich advice as well.   “API Handyman” Arnaud Lauret provides many examples and counterexamples in his book [Lauret 2019] and the online API Stylebook. Applying this refactoring might be the result of an API review, and a new name then has to be decided upon. “A Checklist for API Design Review” is available online.   The blog post “A Definition of Done for Architectural Decisions” presents five criteria to assess whether a decision such as a name change has been elaborated upon sufficiently: evidence, criteria, agreement, documentation, and realization/review.   References             Benner, Tom. 2023. Naming Things: The Hardest Problem in Software Engineering. Independently published.               Dustin Boswell, Trevor Foucher. 2011. The Art of Readable Code. O’Reilly Media, Inc.               Fairbanks, G. 2010. Just Enough Software Architecture: A Risk-Driven Approach. Marshall &amp; Brainerd.               Fowler, Martin. 2018. Refactoring: Improving the Design of Existing Code. 2nd ed. Addison-Wesley Signature Series (Fowler). Boston, MA: Addison-Wesley.               Hermans, F. 2021. The Programmer’s Brain: What Every Programmer Needs to Know about Cognition. Manning.               Lauret, Arnaud. 2019. The Design of Web APIs. Manning.                        Step 3 applies to all refactorings and is part of the general Test, Explain, Let Know and Learn (TELL) activities to enact and evaluate refactorings. TELL is featured on the Content Overview page. &#8617;                  Example: how should we name and structure the attribute to contain a user address? Easy, use the already existing definition https://schema.org/PostalAddress. &#8617;           ","categories": [],
        "tags": [],
        "url": "/refactorings/renamerepresentationelement",
        "teaser": null
      },{
        "title": "Segregate Commands from Queries",
        "excerpt":"also known as: Introduce Command Query Responsibility Segregation (CQRS)   Context and Motivation   An endpoint cohesively bundles all operations dealing with a particular domain concept. Some of these operations modify the application state on the API provider side, while others only retrieve data. Some but not all read operations (following the Retrieval Operation pattern [Zimmermann et al. 2022]) offer declarative query parameters and return rich, multi-valued response structures, causing provider-side workload.   As an API provider, I want to serve queries and process commands separately so that I can optimize the respective read and write model designs independently.   This distinction between commands and queries is known as the Command Query Separation (CQS) principle by Meyer [1997]. CQS states that every method in an object-oriented program should either be a command that performs an action and thus changes state or a query that returns data to the caller, but not both.   Stakeholder Concerns      #performance and #scalability   Computationally expensive workloads such as loading data from data stores, filtering, and formatting it, and high data volumes may make certain operations expensive. Such complex query operations should not slow down cheaper operations exposed by the same endpoint (for example, atomic updates of single attribute values).   #agility and #development-velocity   Read operations and write operations may evolve at different speeds. For example, data analytics queries may often change, driven by client demand and insights just gained, while commands to modify master data might only change with major releases, if at all.   #flexibility to change the API vs. #simplicity   Keeping the read and write operations of an endpoint together is easy to understand and brings functional endpoint cohesion. A separation of these types of operations increases the ability to change them independently from each other; this can then happen more flexibly and more frequently.   #security, #data-privacy   Read and write operations might have different protection needs. Few user roles, for instance, are usually authorized to update master data; many or all user roles may read it. If there are two separate endpoints for read and write access, it might be easier to fine-tune the Confidentiality, Integrity, and Availability (CIA) rules and related compliance controls. See the OWASP API Security Top 10 for risks and related advice on API security.   Initial Position Sketch   The operations an API endpoint offers can be sorted into four categories, depending on whether they read/write state. Each target quadrant is represented by a “Pattern for API Design” [Zimmermann et al. 2022], as shown in Figure 1.   {.resize-half}   Figure 1: The combination of reading and writing state leads to four different operation responsibilities.      Computation Functions derive a result solely from the client input, neither reading nor writing server-side state.   State Creation Operations initialize some new state at the API endpoint (for instance, by creating an implementation resource such as a customer record). A minimal amount of state can be read, for example, to ensure the uniqueness of identifiers.   Retrieval Operations are read-only queries that clients use to fetch data.   State Transition Operations update the server-side state, including full or partial replacement and deleting of state.   These operations are often implemented as CRUD (create, read, update, delete) resources, shown in Figure 2.      Figure 2: Segregate Commands from Queries: Initial Position Sketch. Commands and Queries in Same Endpoint   The target of the refactoring is an endpoint, such as an Information Holder Resource [Zimmermann et al. 2020] that offers both state-writing and state-reading operations. These operations can be realized by HTTP verbs/methods such as POST, GET, PUT, PATCH, and DELETE, supported by an HTTP resource that a URI identifies.   Design Smells      High latency/poor response time   Poor performance may be caused by too tight operation coupling. Expensive queries slow down the execution of write operations (for instance, operations performing state creation or transition). Transactional isolation is insufficient.   Feature/release inertia a.k.a. stale roadmap   An endpoint provides both read and write operations; there might be many read, but only few write operation calls. These types of operations evolve at different speeds; possibly, different development teams are responsible for them. For instance, new query options in a customer relationship management application may be introduced in every two-week iteration in response to frequently arriving customer inquiries and client insights. In contrast, commands evolve with a frequency imposed by a master data management or Enterprise Resource Planning (ERP) package in the backend. The operations also differ in the amount of design and test work required; write operations change state and, therefore may have nontrivial “given” preconditions and “then” postconditions and require consistency management. The conceptual integrity of the endpoint and all of its read and write operations has to be preserved during each evolution step. As a result, it takes longer than desired to introduce new features, new queries in particular.   Too coarse-grained security or data privacy rules   The security and data protection requirements of commands and queries differ. They are specified on the endpoint rather than the operation level. Hence, generalization has to take place that bears risks such as under-specification and over-engineering.   Instructions   Command Query Responsibility Segregation (CQRS) is an architectural pattern that increases flexibility but adds complexity. It can be introduced in the following steps:      Classify and group endpoint operations by their purpose and impact on provider-side state: read-only, write-only, read-write, neither-read-nor-write.   Apply the Extract Operation refactoring to move the read-only operations to a new endpoint, the Read Model API.   Adjust the API implementation to match the outcome of Steps 1 and 2. Consciously decide for a data store serving both endpoints, the new Read Model API and the already existing endpoint that has become a Write Model API.   (Optional) Distribute the data store. When distributing data stores, choose suited data replication and consistency management solutions (for example, how current/fresh should the replicated data be?). Include all data stores in the backup and recovery strategy [Pardon, Pautasso, and Zimmermann 2018].   Test “sunny day scenario” as well as “edge” cases and error situations such as slow and temporarily failing network and replication conflicts.   Update the API Description, including the technical API contract and supporting documentation.   Provide teaching material that covers migration from the old domain concept-oriented API to the new command-query API: What has to be changed in the API client? How do the Service Level Agreements change?   The operation responsibility Computation Function neither reads nor writes provider-side application state.1 Such operations may appear in command endpoints as well as query endpoints; they might also go to separate stateless endpoints, yielding a “Command Computation Responsibility Segregation” variant of CQRS.   The messages and operations stay the same when applying this refactoring. However, the resource address might change. An intermediary such as an API Gateway [Richardson 2018], Service Registry, or Version Mediator can be used to preserve backward compatibility by mapping or forwarding messages.   Target Solution Sketch (Evolution Outline)   After applying the refactoring, shown in Figure 3, two distinct endpoints/resources implement the API operations. One is a Processing Resource handling the commands, and the other is an Information Holder Resource handling the queries.      Figure 3: Segregate Commands from Queries: Target Solution Sketch. Commands and Queries in Separate Endpoints   Example(s)   The following example shows an introduction of CQRS (notation: Context Mapper DSL (CML)):   Service PublicationManagementFacade {     // a state creation/state transition operation:     @PaperId add(@PublicationEntryDTO newEntry);          // retrieval operations:     @PublicationArchive dumpPublicationArchive();     Set&lt;@PublicationEntryDTO&gt;         lookupPublicationsFromAuthor(String writer);     String renderAsBibtex(@PaperId paperId);          // computation operations (stateless):     String convertToBibtex(@PublicationEntryDTO entry);    This single publication management endpoint can be separated into two in this API design:   Service PublicationManagementCommandFacade {     // a state creation/state transition operation:     @PaperId add(@PublicationEntryDTO newEntry);          // computation operations (stateless):     String convertToBibtex(@PublicationEntryDTO entry);  }  Service PublicationManagementQueryFacade {     // retrieval operations:     @PublicationArchive dumpPublicationArchive();     Set&lt;@PublicationEntryDTO&gt;         lookupPublicationsFromAuthor(String writer);     String renderAsBibtex(@PaperId paperId); }   This API design achieves command-query segregation at the expense of distributing the two operations related to BibTeX to two different endpoints. Consequently, the two endpoints are coupled from a domain design standpoint (to some extent).   Hints and Pitfalls to Avoid   When deciding to separate commands from queries by introducing the CQRS pattern:      Replicate data as needed. Decide between strict and eventual consistency consciously.   Be aware of the implications of the Backup Availability Consistency (BAC) theorem [Pardon, Pautasso, and Zimmermann 2018]. The BAC theorem states that it is not possible to backup and restore across services consistently without degrading availability.   Acknowledge that read models and event messages sent as Data Transfer Objects (DTOs) over APIs increase the data coupling between clients and providers. If multiple clients use the same DTOs, they might indirectly also be coupled consequently.2   Consider asynchronous, queue-based messaging to update the read model after a change to the write/command model caused by an API command or a backend activity. This integration style supports throttling and is able to guarantee message delivery (depending on the quality-of-service properties chosen for a particular queue).   Consider applying Event Sourcing as one of several options when segregating commands from queries. An event source stores a series of state changes in chronological order but does not store the resulting final/current state. In such designs, it often makes sense to take snapshots of the current state periodically or upon client request; such snapshots can then be stored separately from the events and provided to clients via additional calls to API operations.   Related Content   This pattern refines Extract Operation in the context of CQRS. Hence, Merge Endpoints reverts it. Introduce Pagination and Add Wish List might be alternative options to improve query performance. The Introduce Data Transfer Object refactoring explains DTO usage.   Information Holder Resources of various types are related patterns that may benefit from command-query segregation. In “Patterns for API Design” [Zimmermann et al. 2022], queries are represented as Retrieval Operations; commands are State Creation Operations or State Transition Operations.   Michael Ploed provides a comprehensive introduction to CQRS and event sourcing on slideshare. A presentation video by Michael Ploed is available as well. Also see an online article by Udi Dahan for examples and a discussion of pros and cons. The Context Mapper website provides a tutorial, “Event Sourcing and CQRS Modeling in Context Mapper.”   References             Evans, Eric. 2003. Domain-Driven Design: Tacking Complexity in the Heart of Software. Addison-Wesley.               Meyer, Bertrand. 1997. Object-Oriented Software Construction (2nd Ed.). USA: Prentice-Hall, Inc.               Pardon, Guy, Cesare Pautasso, and Olaf Zimmermann. 2018. “Consistent Disaster Recovery for Microservices: The BAC Theorem.” IEEE Cloud Computing 5 (1): 49–59. https://doi.org/10.1109/MCC.2018.011791714.               Richardson, Chris. 2018. Microservices Patterns. Manning.               Zimmermann, Olaf, Daniel Lübke, Uwe Zdun, Cesare Pautasso, and Mirko Stocker. 2020. “Interface Responsibility Patterns: Processing Resources and Operation Responsibilities.” In Proc. Of the European Conference on Pattern Languages of Programs. EuroPLoP ’20. Online.               Zimmermann, Olaf, Mirko Stocker, Daniel Lübke, Uwe Zdun, and Cesare Pautasso. 2022. Patterns for API Design: Simplifying Integration with Loosely Coupled Message Exchanges. Addison-Wesley Signature Series (Vernon). Addison-Wesley Professional.                        unlike State Creation Operation, Retrieval Operation, and State Transition Operation &#8617;                  This cannot be avoided entirely in any Published Language [Evans 2003] in an API; the coupling still exists but becomes less obvious when commands and queries are separated (as they still work on the same domain concepts). If the two endpoints evolve autonomously (independently of each other, that is), the models will eventually deviate further and further (which to some extent is desired). Over time, this may cause technical debt and hidden dependencies that counter the original motivation of the pattern and the refactoring. If this happens, the inverse Merge Endpoints refactoring may be applied. &#8617;           ","categories": [],
        "tags": [],
        "url": "/refactorings/segregatecommandsfromqueries",
        "teaser": null
      },{
        "title": "Split Operation",
        "excerpt":"also known as: Decompose API Call   Context and Motivation   An API with endpoints and operations has been defined and implemented. Some operations acquire multiple responsibilities because new capabilities are added to the API. For example, an operation might accept several types of request messages that lead to different parts of execution logic in the API implementation.   As an API provider, I want to offer endpoints that expose operations with distinct responsibilities so that the API is easy to understand and use for client developers and can be modified rapidly and flexibly by provider developers.   Stakeholder Concerns      #single-responsibility-principle and #understandability   An API that offers endpoints with operations that follow the single responsibility principle is easier to understand for clients because operations are focused and do not offer extra capabilities that not all clients use. The API provider team also benefits from a lower complexity of the implementation.   #maintainability, #evolvability   Operations that follow the single responsibility principle are easier to maintain and evolve, for instance, because their API provider only has a single reason to change their request and response message structure and content when evolving the API. Changing an operation that does many things is unnecessarily complex if the change only affects certain aspects of the operation; unexpected side effects may occur. For example, deprecating the operation is more complicated if it has multiple stakeholders.   #testability   Test cases have to cover all, and all combinations of, the many things the operation is in charge of. Testing such an operation is more complex than testing a single-responsibility operation.   #security   Access restrictions of an API can be implemented on different levels: whole API, per endpoint, individual operations, or even depending on the executed control flow paths in the implementation of operations or the data accessed. Securing an operation that does many things is complex. The access control list must include the superset of all involved participants; changing it has a significant impact.   Initial Position Sketch   The implementation logic chosen by the operation depends on data sent by the client (represented by the Metadata Element). This branching parameter could take the form of a boolean parameter, an enumeration, or any other value obtained from the request message or the resource state. For example, the request body could contain the data of a resource representation, and depending on whether that resource is already known to the API, it is either updated or created. Figure 1 visualizes this initial position.      Figure 1: Split Operation: Initial Position Sketch. A client sends a request (1) that includes metadata, such as an enumeration value or boolean flag. The provider uses this information to steer the control flow, choosing between operations (Op1, Op2) to execute and compose a response message (2). This metadata might be optional, as shown in the second exchange (Request 3, Response 4). The provider might choose a default operation or even return an error message in that case.   This refactoring targets an endpoint implementation and its request and response messages.   Design Smells      Role and/or responsibility diffusion and low cohesion   An operation does (too) many things. Clients have to understand all these things to use the operation correctly. Its request message is rather deeply structured and may contain optional, generic, or variable parts to express diverse input options. This complexity may lead to errors and a degraded developer experience. The internal cohesion of the operation is low.   Combinatorial explosion of input options   Boolean parameters or other flags that determine the execution path lead to a combinatorial explosion of possibilities. Explaining these options bloats the API Description and is problematic for the client, who has to understand this complex option space to prepare valid requests, and the provider, who has to validate and process the parameter handling. API testing on the client and provider side is also complicated.   Change log jitter or commit chaos   The operation has been, and continues to be, modified frequently, according to the commit logs kept by the version control system. Frequent changes may indicate that the operation has too many responsibilities and is not focused enough.   Instructions   To split an operation, the following steps apply:      Copy the operation implementation, remove the branching metadata parameter (in the copy), and delete the part of the implementation that is no longer needed for the new operation.   (Optional) If there is common code in the two operations, extract it into a new method and call it from both operations. This step is optional because the shared code might be small enough to be inlined in both operations (consider the Rule of Three of refactoring1 by Fowler [2018]).   Expose the new operation to clients. Depending on the underlying technology, this can be non-trivial. When using HTTP, choosing a different (previously unused) verb might be appropriate for the new operation. See Singjai et al. [2021] for a collection of patterns on designing API operations.   Copy the tests covering the implementation parts that now reside in the copy. Remove the obsolete branching metadata parameter from the tests, adjust the test data and assertions, and ensure the tests pass.   Include the newly created operation in the API Description. Inform clients that a new operation now covers the previous behavior. The decision logic previously encoded on the provider side must now be implemented on the client side instead of just passing a branching parameter.   If access to the original operation was restricted to specific clients, apply the same security rules to the new operation.   If necessary, to avoid breaking changes, mark the branching metadata parameter as deprecated or immediately remove it, along with any unused code in the original operation implementation. Note that this decision depends on the lifecycle guarantees given to clients, as documented as one of the Evolution Patterns.   For backward compatibility, a Content-Based Router [Hohpe and Woolf 2003] can forward requests to the correct operation.   Target Solution Sketch (Evolution Outline)   After the refactoring, the behavior of the original operation is cut into two parts, which are implemented by two distinct operations (“Op1” and “Op2”).      Figure 2: Split Operation: Target Solution Sketch. Instead of using metadata parameters to steer the provider behavior, two distinct operations (Op1, Op2) are offered. Clients can invoke one (Request 1, Response 2) or the other (Request 3, Response 4).   Splitting the operation into two (or more) distinct operations makes each one easier to use and maintain. No provider-side dispatching or branching logic is required anymore. As a downside, the client implementation might become more complex because it has to decide which operation to call. The amount of request-response message exchanges usually stays the same, though.   Example(s)   The following example from a construction management API shows a Spring Boot implementation of an endpoint that offers an updateConstruction operation to modify the data of a particular building site, specified by the id parameter:   @PutMapping(\"/constructions/{id}\") public ResponseEntity&lt;Construction&gt; updateConstruction(     @PathVariable(value = \"id\") Long id,     @PathVariable(value = \"partial-update\") Boolean partial,     @NotNull @RequestBody Construction construction) {      if (!constructionRepository.existsById(id)) {         throw new ResponseStatusException(             HttpStatus.BAD_REQUEST, \"Entity not found\");     }      Construction result;     if (partial) {         result = constructionRepository             .findById(construction.getId())             .map(existingConstruction -&gt; {                 if (construction.getName() != null) {                     existingConstruction.setName(                         construction.getName());                 }                 // repeat this for all attributes                 return existingConstruction;             })             .map(constructionRepository::save).get();     } else {         result = constructionRepository.save(construction);     }      return ResponseEntity.ok().body(result); }   The operation takes a boolean partialUpdate parameter. If it is set to true, the attributes that the client provides in the request body are overwritten. If partialUpdate is false, the entire entity is replaced, as shown in the else block.   HTTP provides the PATCH verb to represent partial updates (whereas PUT methods are supposed to replace the entire resource). So we can move the “patch” parts of the operation to a new operation:   @PatchMapping(\"/constructions/{id}\") public ResponseEntity&lt;Construction&gt; updatePartially(     @PathVariable(value = \"id\") Long id,     @NotNull @RequestBody Construction construction) {      if (!constructionRepository.existsById(id)) {         throw new ResponseStatusException(             HttpStatus.BAD_REQUEST, \"Entity not found\");     }      Construction result = constructionRepository         .findById(construction.getId())         .map(existingConstruction -&gt; {             if (construction.getName() != null) {                 existingConstruction.setName(                     construction.getName());             }             // repeat this for all attributes             return existingConstruction;         })         .map(constructionRepository::save).get();      return ResponseEntity.ok().body(result); }   Note the PatchMapping annotation on the updatePartially operation that was added. The old updateConstruction operation has become much simpler now (the + and - stand for added and removed lines, respectively):    @PutMapping(\"/constructions/{id}\")  public ResponseEntity&lt;Construction&gt; updateConstruction(      @PathVariable(value = \"id\") Long id, -    @PathVariable(value = \"partial-update\") Boolean partial,      @NotNull @RequestBody Construction construction) {     if (!constructionRepository.existsById(id)) {          throw new ResponseStatusException(              HttpStatus.BAD_REQUEST, \"Entity not found\");     } +   Construction result =  +       constructionRepository.save(construction); -   if (partial) { -       result = constructionRepository -           .findById(construction.getId()) -           .map(existingConstruction -&gt; { -               if (construction.getName() != null) { -                   existingConstruction.setName( -                       construction.getName()); -               } -               // repeat this for all attributes -               return existingConstruction; -           }) -           .map(constructionRepository::save).get(); -   } else { -       result = constructionRepository.save(construction); -   }     return ResponseEntity.ok().body(result);  }   In this example, no operation in the endpoint had a PatchMapping so far. If this had been the case, we would have had to introduce a new endpoint for the split-off operation and apply Move Operation to move either operation to the new endpoint.   Even though the method name updateConstruction is an implementation detail and not exposed to API clients, it could also be renamed. For example, replaceConstruction would fit better with the new responsibility of the method.   Hints and Pitfalls to Avoid   When using HTTP, follow the conventions of the protocol. For example, a PUT request should be idempotent, meaning that the result of sending such a request is the same whether it has been sent exactly once or multiple times. In contrast, a POST request is not necessarily idempotent. Sending it multiple times might lead to incorrect provider-side state, such as duplicated data. Not following such conventions confuses clients and may cause API usage bugs.   HTTP redirections provide a technical solution for informing clients about the new operation [Fielding and Reschke 2014]. This approach only works if the URI changes. Using redirects to tell clients to use another HTTP verb is not possible.   With respect to security concerns, the split-off operation should probably be accessible to the same clients as the original operation if authentication and authorization are required.   Possible impacts on Rate Limits, monitoring, caching, and other aspects of the API should be considered as well.   Related Content   Merge Operations is the inverse refactoring. Add Wish List can also combine two Retrieval Operations that return related data.   Once an operation has been split into two, one can also be moved to a different endpoint with the Move Operation refactoring.   When refactoring the API implementation, the Extract Method [Fowler 2018] refactoring is eligible.   The correct use of HTTP verbs and many other REST implementation hints are explained in the RESTful Web Services Cookbook [Allamaraju 2010].   References             Allamaraju, Subbu. 2010. RESTful Web Services Cookbook. O’Reilly.               Fielding, Roy T., and Julian Reschke. 2014. “Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content.” Request for Comments. RFC 7231; RFC Editor. https://doi.org/10.17487/RFC7231.               Fowler, Martin. 2018. Refactoring: Improving the Design of Existing Code. 2nd ed. Addison-Wesley Signature Series (Fowler). Boston, MA: Addison-Wesley.               Hohpe, Gregor, and Bobby Woolf. 2003. Enterprise Integration Patterns: Designing, Building, and Deploying Messaging Solutions. Addison-Wesley.               Singjai, Apitchaka, Uwe Zdun, Olaf Zimmermann, Mirko Stocker, and Cesare Pautasso. 2021. “Patterns on Designing API Endpoint Operations.” Virtual: ACM; ACM.                        Not to be confused with the Rule of Three of the patterns community: Only call it a pattern if there are at least three known uses (https://wiki.c2.com/?RuleOfThree). &#8617;           ","categories": [],
        "tags": [],
        "url": "/refactorings/splitoperation",
        "teaser": null
      },{
        "title": "Tighten Evolution Strategy",
        "excerpt":"also known as: Increase Lifetime Guarantee, Strengthen/Stretch Lifetime Commitment   Context and Motivation   An API runs in production. Its compatibility and extensibility characteristics are expressed in the form of one or more evolution patterns, including Experimental Preview, Aggressive Obsolescence, Limited Lifetime Guarantee, and Two in Production. These patterns differ in the way and the frequency the provider is able to introduce breaking changes.1   As an API provider, I want to expand and extend the lifetime, stability, and support guarantees of an API endpoint with its elements (operations, messages) so that existing and new clients can use the API longer.   Stakeholder Concerns      #maintainability   During their lifetimes, API versions have to be supported and evolved. Security patches are applied, bugs are fixed, and so on. The effort for such activities might not be proportional to the guaranteed lifetime but certainly is related and somewhat coupled to it. Related (sub-)concerns are compatibility and extensibility.   #cost   Effort comes with cost. Operational and capital expenditure, including budget for training, evolution (including documentation), and operations increase as lifetime guarantees are extended.   #flexibility   The more lifetime guarantees are given and the longer the committed time spans are, the less freedom remains with respect to changing product vision and corresponding API designs (for instance, the role of an API in the organization).   According to “Interface Evolution Patterns — Balancing Compatibility and Extensibility across Service Life Cycles”, evolution patterns are confronted with desired qualities such as:      Enhancing compatibility and developer experience   Allowing the provider and the client to follow different life cycles, e.g., a provider can roll out a new API version without breaking existing clients   Minimizing changes to the client forced by API changes   Making it possible for the provider to improve and extend the API and change it to accommodate new requirements   Guaranteeing that API changes do not lead to semantic ‘misunderstandings’ between client and provider   Minimizing the maintenance effort to support old clients [Lübke et al. 2019]   Initial Position Sketch   The following provider specification does not yet define any life cycle guarantee (notation: MDSL):   API provider SampleAPIProvider1    offers SomeDemoContract      at endpoint location \"http://www.testdomain.io:80/path/subpath\"     via protocol HTTP binding resource SampleAPIProvider1Resource   Clients are not clear about what they can expect from the provider:      Tighten Evolution Strategy: Initial Position Sketch   This “refactoring”2 targets API endpoints and their API Descriptions (including business and technical contract between a provider and its clients). It may also affect any Service Level Agreement (SLA) that accompanies or is part of such an API Description.   Design Smells      Lack of trust and confidence   Client developers decide not to use the API because it comes across as unstable and subject to change often (or disappear).   Client community smaller than expected   The provider receives less client traffic than expected and hoped for. Client feedback indicates that the API functionality is appreciated, but API usage is not considered a viable design option due to missing stability and support guarantees.   Instructions   The following transitions and combinations of evolution patterns to increase guarantees make sense:      Upgrade from no guarantee to Experimental Preview.   Transition from Experimental Preview to Aggressive Obsolescence.   Become more assertive and switch from Aggressive Obsolescence to Limited Lifetime Guarantee.   Change from any other life cycle pattern to Two In Production.   Refine from Two In Production to its N in Production variant (with N equal to or greater than three).      Target Solution Sketch (Evolution Outline)   The life cycle guarantee is not an architectural element but a documentation item first and foremost.      Tighten Evolution Strategy: Target Solution Sketch   Some Interface Definition Languages (IDLs) have dedicated support for such Metadata Elements (for instance, OpenAPI and MDSL); in other documentation formats, lifecycle and version information usually goes into more general description fields or comments.   Example(s)   When applying this refactoring, an MDSL contract changes from:   API provider SampleAPIProvider1 version 1.0.0   offers SomeDemoContract      at endpoint location \"http://www.testdomain.io:80/path/subpath\"     via protocol HTTP    binding      resource SampleAPIProvider1Resource   provider governance AGGRESSIVE_OBSOLESCENCE   to:   API provider SampleAPIProvider1 version 2.0.0   offers SomeDemoContract      at endpoint location \"http://www.testdomain.io:80/path/subpath\"     via protocol HTTP   binding      resource SampleAPIProvider1Resource   provider governance LIMITED_LIFETIME_GUARANTEE   The Evolution Patterns on the MAP website feature real-world known uses of the patterns (and their introduction over time).   Hints and Pitfalls to Avoid   Apply this refactoring when existing clients complain, for instance about a lack of planning horizon and effort to catch up with API updates.   When applying the refactoring, perform a cost-benefit analysis. Communicate changes early, clearly, and continuously. Minimize incompatible changes and apply Semantic Versioning when releasing.   Do not over-commit but find a balance between conflicting concerns, for instance those of external stakeholders and those of the API development team.   Related Content   This refactoring is the opposite of Relax Evolution Strategy.   Introduce Version Mediator can be applied if lifetime guarantees are weakened and some clients are unable to migrate to a newer, incompatible version. And Introduce Version Identifier might have to be applied before this one so that clients can learn about versions and their (in-)compatibilities.   MDSL specifications have an optional SLA section and a provider governance element in the API provider part. This is explained on the MDSL website.   References             Lübke, Daniel, Olaf Zimmermann, Cesare Pautasso, Uwe Zdun, and Mirko Stocker. 2019. “Interface Evolution Patterns: Balancing Compatibility and Extensibility Across Service Life Cycles.” In Proceedings of the 24th European Conference on Pattern Languages of Programs. EuroPLop ’19. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3361149.3361164.                        Alternatively (but only justified in rare cases), the provider might decide not to do so at all and guarantee an Eternal Lifetime Guarantee. &#8617;                  We use double quotes here because this design tactic actually does not change the API structure, but a quality property of it. Note that tightening is the opposite of relaxing; a reverse “refactoring” Relax Evolution Strategy exists as well. &#8617;           ","categories": [],
        "tags": [],
        "url": "/refactorings/tightenevolutionstrategy",
        "teaser": null
      }]
